---
title: 'Graph convolutional neural networks'
date: 2023-09-10
permalink: /posts/gcn/
tags:
  - tutorial
  - deep learning
  - machine learning
---

_This post is currently under construction_ 

Introduction
------------

Graphs are ubiqitous mathematical objects that describe a set of relationships between entities; however, they are challenging to model with traditional machine learning methods, which require that the input be represented as a tensor. Graphs break this paradigm due to the fact that the order of edges and nodes are arbitrary -- that is, they are _permutation invariant_ -- and the model must be capable of accomodating this feature.  In this post, we will discuss graph convolutional networks (GCNs) as presented by [Kipf and Welling (2016)](https://arxiv.org/abs/1609.02907): a class of neural network designed to operate on graphs. As their name suggestions, graph convolutional neural networks can be understood as performing a convolution in the same way that traditional convolutional neural networks (CNNs) are performing a convolution-like operation (i.e., [cross correlation](https://en.wikipedia.org/wiki/Cross-correlation)) when operating on image data. This analogy is depicted below:

<br>

<center><img src="https://raw.githubusercontent.com/mbernste/mbernste.github.io/master/images/GCN_vs_CNN_overview.png" alt="drawing" width="500"/></center>

In this post, we will discuss the intution behind the GCN and how it is similar and different to the CNN. We will conclude by presenting a case-study training a GCN to classify molecule toxicity. 

Inputs and outputs of a GCN
---------------------------

Fundamentally, a GCN takes as input a graph together with a set of [feature vectors](https://en.wikipedia.org/wiki/Feature_(machine_learning)) where each node is associated with its own feature vector. The GCN is then composed of a series of graph convolutional layers (to be discussed in the next section) that iteratively transform the feature vectors at each node. The output is then the graph associated with output vectors associated with each node. These output vectors can be (and often are) of different dimension than the input vectors. This is depicted below:

<center><img src="https://raw.githubusercontent.com/mbernste/mbernste.github.io/master/images/GCN_output_vectors_per_node.png" alt="drawing" width="800"/></center>

If the task at hand is a "node-level" task, such as performing [classification](https://en.wikipedia.org/wiki/Statistical_classification) on the nodes, then these per-node vectors would be treated as the model's final outputs. In the example case of node-level classification, these output vectors could encode the probabilities that each node is associated with each class. 

Alternatively, we may be interested in performing a "graph-level" task, where instead of building a model that produces an output per node, we are interested in task that requires an output over the graph as a whole. For example, we may be interested in classifying whole graphs rather than individual nodes. In this scenario, the per-node vectors could be fed, collectively, into another neural network (such as a simple [multilayer perceptron](https://en.wikipedia.org/wiki/Multilayer_perceptron)), that operates on all them to produce a single output vector. This scenario is depicted below:

<center><img src="https://raw.githubusercontent.com/mbernste/mbernste.github.io/master/images/GCN_output_vectors_per_graph.png" alt="drawing" width="800"/></center>

Note, GCNs can also perform "edge-level" tasks, but we will not discuss this here. See [this article by Sanchez-Lengeling  _et al_. (2021)](https://distill.pub/2021/gnn-intro/) for a discussion on how GCNs can perform various types of tasks with graphs. 

In the next sections we will dig deeper into the graph convolutional layer and discuss how the per-node embeddings can be combined for producing graph-wide output.


The graph convolutional layer
-----------------------------

GCNs are composed of layers of modules called **graph convolutional layers** in a similar way that traditional CNNs are composed of convolutional layers. Each convolutional layer takes as input each node's vector from the previous layer (in the case of the first layer this would be the input feature vectors) and produces a corresponding output vector for each node. To do so, the graph convolutional layer pools the vectors from each node's neighbors as depicted below:

<center><img src="https://raw.githubusercontent.com/mbernste/mbernste.github.io/master/images/GCN_conv_layer_message_pass_1.png" alt="drawing" width="800"/></center>

<br>

In the schematic above, node A's vector, denoted $\boldsymbol{x}_A$ is pooled/aggregated with the vectors of its neighbors, $\boldsymbol{x}_B$ and $\boldsymbol{x}_C$. This pooled vector is then transformed/updated to form node A's vector in the next layer, denoted $\boldsymbol{h}_A$. This same procedure is carred out over every node. Below we show this same procedure, but on node D, which entails aggregating $\boldsymbol{x}_D$ with $\boldsymbol{x}_B$:

<center><img src="https://raw.githubusercontent.com/mbernste/mbernste.github.io/master/images/GCN_conv_layer_message_pass_2.png" alt="drawing" width="800"/></center>

<br>

This procedure is often called **message passing** since each node is "passing" its vector to its neighbors to update their vector. Each node's "message" is the vector that is associated with it.

Now, how exactly does a GCN perform the aggregation and update? Moreover, where are the model's parameters in this scheme? To answer these questions, we'll now dig into the mathematics of a GCN. Let $\boldsymbol{X} \in \mathbb{R}^{n \times d}$ be the features corresponding to the nodes where $n$ is the number of nodes and $d$ is the number of features. That is, row $i$ of $\boldsymbol{X}$ stores the features of node $i$. Let $\boldsymbol{A}$ be the adjacency matrix of the graph where

$$\boldsymbol{A}_{i,j} := \begin{cases} 1,& \text{if there is an edge between node} \ i \ \text{and} \ j \\ 0, & \text{otherwise}\end{cases}$$

Note the matrices $\boldsymbol{X}$ and $\boldsymbol{A}$ are the two inputs required as input to a GCN for a given graph. The graph convolution layer can then be expressed as function on these two inputs that outputs a matrix representing the vectors associated with each node at the next layer. This function is given by:

$$f(\boldsymbol{X}, \boldsymbol{A}) := \sigma\left(\boldsymbol{D}^{-1/2}(\boldsymbol{A}+\boldsymbol{I})\boldsymbol{D}^{-1/2} \boldsymbol{X}\boldsymbol{W}\right)$$

where,

$$\begin{align*}\boldsymbol{A} \in \mathbb{R}^{n \times n} &:= \text{The adjacency matrix} \\ \boldsymbol{I} \in \mathbb{R}^{n \times n} &:= \text{The identity matrix} \\ \boldsymbol{D} \in \mathbb{R}^{n \times n} &:= \text{The degree matrix of } \ \boldsymbol{A}+\boldsymbol{I} \\ \boldsymbol{X} \in \mathbb{R}^{n \times d} &:= \text{The input data (i.e., the per-node feature vectors)} \\ \boldsymbol{W} \in \mathbb{R}^{d \times w} &:= \text{The layer's weights} \\ \sigma(.) &:= \text{The activation function (e.g., ReLU)}\end{align*}$$

When I first saw this equation I found it to be quite confusing. To break it down, here is what each [matrix multiplication](https://mbernste.github.io/posts/matrix_multiplication/) is doing in this function:

<center><img src="https://raw.githubusercontent.com/mbernste/mbernste.github.io/master/images/GCN_layer_equation_annotated.png" alt="drawing" width="350"/></center>

<br>

Let's examine the first operation: $\boldsymbol{A}+\boldsymbol{I}$. This operation is simply adding ones along the diagonal entries of the adjacency matrix. This is the equivalent of adding self-loops to the graph where each node has an edge pointing to itself. The reason we need this is because when we perform message passing, each node should pass its vector to itself (since each node aggregates its own vector together with its neighbors).

The matrix $\boldsymbol{D}$ is the [degree matrix](https://en.wikipedia.org/wiki/Degree_matrix) of $\boldsymbol{A}+\boldsymbol{I}$. This is a diagonal matrix where element $i,i$ stores the total number of neighboring nodes to node $i$ (including itself). That is, 

$$\boldsymbol{D} := \begin{bmatrix}d_{1,1} & 0 & 0 & \dots & 0 \\ 0 & d_{2,2} & 0 & \dots & 0 \\ 0 & 0 & d_{3,3} & \dots & 0 \\ 
\vdots & \vdots & \vdots & \ddots & \vdots \\ 0 & 0 & 0 & \dots & d_{n,n}\end{bmatrix}$$

where $d_{i,i}$ is the number of neighbors of node $i$. 

The matrix $\boldsymbol{D}^{-1/2}$ is the matrix formed by taking the reciprocal of the square root of each entry in $\boldsymbol{D}$. That is,

$$\boldsymbol{D}^{-1/2} := \begin{bmatrix}\frac{1}{\sqrt{d_{1,1}}} & 0 & 0 & \dots & 0 \\ 0 & \frac{1}{\sqrt{d_{2,2}}} & 0 & \dots & 0 \\ 0 & 0 & \frac{1}{\sqrt{d_{3,3}}} & \dots & 0 \\ 
\vdots & \vdots & \vdots & \ddots & \vdots \\ 0 & 0 & 0 & \dots & \frac{1}{\sqrt{d_{n,n}}}\end{bmatrix}$$

As we will discuss in the next section, left and right multiplying $\boldsymbol{A}+\boldsymbol{I}$ by $\boldsymbol{D}^{-1/2}$ can be viewed as "normalizing" the adjacency matrix. We will discuss what we mean by "normalizing" in the next section and why this is an important step; however, for now, the important point to relealize is that, like $\boldsymbol{A}$, the matrix $\boldsymbol{D}^{-1/2}(\boldsymbol{A}+\boldsymbol{I})\boldsymbol{D}^{-1/2}$ will also only have a non-zero entry at element $i,j$ if nodes $i$ and $j$ are adjacent. Specifically, 

$$\tilde{\boldsymbol{A}}_{i,j} := \begin{cases} \frac{1}{\sqrt{d_{i,i} d_{j,j}}} ,& \text{if there is an edge between node} \ i \ \text{and} \ j \\ 0, & \text{otherwise}\end{cases}$$

For now, let's let $\tilde{\boldsymbol{A}}$ denote this normalized matrix. That is,

$$\tilde{\boldsymbol{A}} := \boldsymbol{D}^{-1/2}(\boldsymbol{A}+\boldsymbol{I})\boldsymbol{D}^{-1/2}$$

Then we can simplify the graph convolutional layer function as follows:

$$f(\boldsymbol{X}, \boldsymbol{A}) := \sigma\left(\tilde{\boldsymbol{A}}\boldsymbol{X}\boldsymbol{W}\right)$$

Next, let's turn to the matrix $\tilde{\boldsymbol{A}}\boldsymbol{X}$. This matrix-product is performing the aggregation function that we described previously. That is, for every feature, we take a weighted sum of the features of the adjacent nodes where the weights are determined by $\tilde{\boldsymbol{A}}$. 

<center><img src="https://raw.githubusercontent.com/mbernste/mbernste.github.io/master/images/GCN_aggregation_matrices.png" alt="drawing" width="700"/></center>

<br>

<center><img src="https://raw.githubusercontent.com/mbernste/mbernste.github.io/master/images/GCN_as_neural_net.png" alt="drawing" width="800"/></center>



Normalizing the adjacency matrix
--------------------------------

Let's take a closer look at the normalized adjacency matrix $ \tilde{\boldsymbol{A}} := \boldsymbol{D}^{-1/2}(\boldsymbol{A}+\boldsymbol{I})\boldsymbol{D}^{-1/2}$. What is the intuition behind this matrix and what do we mean by 
"normalized". 

To understand this normalized matrix, let us first consider what happens in the convolutional layer if we don't perform any normalization and instead naively use the raw adjacency matrix (with ones along the diagonal), $\boldsymbol{A}+\boldsymbol{I}$. For ease of notation let

$$\hat{\boldsymbol{A}} := \boldsymbol{A} + \boldsymbol{I}$$

Then, the graph convolutional layer function without normalization would be:

$$f_{\text{unnormalized}}(\boldsymbol{X}, \boldsymbol{A}) := \sigma(\hat{\boldsymbol{A}}\boldsymbol{X}\boldsymbol{W})$$

In the aggregation step, the aggregated features for node $i$, $\bar{\boldsymbol{x}}_i$ will be given by

$$\begin{align*}\bar{\boldsymbol{x}}_i &= \sum_{j=1}^n \hat{a}_{i,j} \boldsymbol{x}_j \\ &= \sum_{j=1}^n \mathbb{I}(j \in \text{Neigh}(j)) \boldsymbol{x}_j \\ &= \sum_{j \in \text{Neigh}(i)} \boldsymbol{x}_j\end{align*}$$

We see that this aggregation step simply adds together all of the feature vectors of $i$'s adjacent nodes with its own feature vector. A problem becomes apparent: for nodes that have many neighbors, this sum will be large and we will get vectors with large magnitudes. Conversely, for nodes with few neighbors, this sum will result in vectors with small magnitudes. This is not a desirable property! When attempting to train our neural network, each node's vector will be highly dependent on the number of neighbors that surround it and it will be challenging to optimize weights that look for signals in the neighboring nodes that are independent of the number of neighbors. Thus, we need a way to perform this aggregation step so that the aggregated vector for each node is of similar magnitude and is not so dependent on the number of neighbors that each node has.

Recall that the normalized adjacency matrix is given by 

$$\tilde{\boldsymbol{A}}_{i,j} := \begin{cases} \frac{1}{\sqrt{d_{i,i} d_{j,j}}} ,& \text{if there is an edge between node} \ i \ \text{and} \ j \\ 0, & \text{otherwise}\end{cases}$$

Note that the edge weight between adjacent nodes $i$ and $j$ will be smaller if nodes $i$ and $j$ are connected to many nodes, and larger if they are connected to few nodes. We will discuss the intuition into why we perform this normalization in the next section. 


Comparing GCNs and CNNs
-----------------------

In the prior section, we described the graph convolution layer as performing a message passing procedure.  However, there is [another perspective]() from which we can view this process: as performing a convolution operation similar to the convolution-like operation that is performed by CNNs on images (hence the name graph _convolutional_ neural network). Specifically, we can view the message passing procedure instead as the process of passing a **filter** (also called a **kernel**) over each node such that when the filter is centered over a given node, it combines data from the nearby nodes to produce the output vector for that node. In the figure below, we show how a filter is moved over a graph in a similar way that a filter is moved over an image in a traditional CNN: 



Related links
-------------

* **A Gentle Introduction to Graph Neural Networks** [https://distill.pub/2021/gnn-intro/](https://distill.pub/2021/gnn-intro/)
* **Graph Convolutional Networks** [https://tkipf.github.io/graph-convolutional-networks/](https://tkipf.github.io/graph-convolutional-networks/)






