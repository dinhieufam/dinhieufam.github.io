---
title: 'Graph Convolutional Neural Networks'
date: 2023-09-10
permalink: /posts/gcn/
tags:
  - tutorial
  - deep learning
  - machine learning
---

_This post is currently under construction_ 

Introduction
------------

Variational autoencoders (VAEs), introduced by [Kingma and Welling (2013)](https://arxiv.org/abs/1312.6114_), are a class of probabilistic models that find latent, low-dimensional representations of data. VAEs are thus a method for performing [dimensionality reduction](https://en.wikipedia.org/wiki/Dimensionality_reduction) to reduce data down to their [intrinsic dimensionality](https://mbernste.github.io/posts/intrinsic_dimensionality/). 

As their name suggests, VAEs are a type of **autoencoder**. An autoencoder is a model that takes a vector, $\boldsymbol{x}$, compress it into a lower-dimensional vector, $\boldsymbol{z}$, and then decompress $\boldsymbol{z}$ back into $\boldsymbol{x}$. The architecture of an autoencoder can can be visualized as follows:

&nbsp;

<center><img src="https://raw.githubusercontent.com/mbernste/mbernste.github.io/master/images/autoencoder.png" alt="drawing" width="350"/></center>
