---
title: 'Graph convolutional neural networks'
date: 2023-09-10
permalink: /posts/gcn/
tags:
  - tutorial
  - deep learning
  - machine learning
---

_This post is currently under construction_ 

Introduction
------------

Graphs are ubiqitous mathematical objects that describe a set of relationships between entities; however, they are challenging to model with traditional machine learning methods, which require that the input be represented as a tensor. Graphs break this paradigm due to the fact that the order of edges and nodes are arbitrary -- that is, they are _permutation invariant_ -- and the model must be capable of accomodating this feature.  In this post, we will discuss graph convolutional networks (GCNs) as presented by [Kipf and Welling, 2016](https://arxiv.org/abs/1609.02907): a class of neural network designed to operate on graphs. As their name suggestions, graph convolutional neural networks can be understood as performing a convolution in the same way that traditional convolutional neural networks (CNNs) are performing a convolution-like operation (i.e., [cross correlation](https://en.wikipedia.org/wiki/Cross-correlation)) when operating on image data. This analogy is depicted below:

<br>

<center><img src="https://raw.githubusercontent.com/mbernste/mbernste.github.io/master/images/GCN_vs_CNN_overview.png" alt="drawing" width="500"/></center>

In this post, we will discuss the intution behind the GCN and how it is similar and different to the CNN. We will conclude by presenting a case-study training a GCN to classify molecule toxicity. 

Inputs and outputs of a GCN
---------------------------

Fundamentally, a GCN takes as input a graph together with a set of [feature vectors](https://en.wikipedia.org/wiki/Feature_(machine_learning)) where each node is associated with its own feature vector. The GCN is then composed of a series of graph convolutional layers (to be discussed in the next section) that iteratively transform the feature vectors at each node. The output is then the graph associated with output vectors associated with each node. This is depicted below:

<center><img src="https://raw.githubusercontent.com/mbernste/mbernste.github.io/master/images/GCN_output_vectors_per_node.png" alt="drawing" width="800"/></center>

These output vectors could be the final output of the model if the task at hand is a "node-level task". For example, if the task is to classify the nodes of a graph into a set of classes, then these output probabilities could be the probability that each node is associated with each class. Alternatively, these vectors could be fed, collectively, into another neural network, such as a simple [multilayer perceptron](https://en.wikipedia.org/wiki/Multilayer_perceptron), that operates on all them to perform some graph-level task. For example, we may be doing graph-level classification where instead of classifying each node, we are classifying the graph as a whole. This scenario is depicted below:

<center><img src="https://raw.githubusercontent.com/mbernste/mbernste.github.io/master/images/GCN_output_vectors_per_graph.png" alt="drawing" width="800"/></center>

Note, GCNs can also perform edge level tasks as well, but we will not discuss this here. See [this article by Sanchez-Lengeling  et al. (2021)](https://distill.pub/2021/gnn-intro/) for a discussion on these various tasks. 

In the next sections we will dig deeper into the graph convolutional layer and discuss how the per-node embeddings can be combined for producing graph-wide output.


The graph convolutional layer
-----------------------------

Let $\boldsymbol{X} \in \mathbb{R}^{n \times d}$ be the features corresponding to the nodes where $n$ is the number of nodes and $d$ is the number of features. Then, the network's first layer's "features", denoted $\boldsymbol{H}_1 \in \mathbb{R}^{n \times d_1}$, where $d_1$ is the number of features in the first layer, is computed as:

$$\boldsymbol{H}_1 := \sigma\left(\hat{\boldsymbol{D}}^{-1/2}(\boldsymbol{A}+\boldsymbol{I})\hat{\boldsymbol{D}}^{-1/2} \boldsymbol{X}\boldsymbol{W}_1\right)$$

where,

$$\begin{align*}\boldsymbol{A} \in \mathbb{R}^{n \times n} &:= \text{The adjacency matrix} \\ \boldsymbol{I} \in \mathbb{R}^{n \times n} &:= \text{The identity matrix} \\ \hat{\boldsymbol{D}} \in \mathbb{R}^{n \times n} &:= \text{The degree matrix of } \ \boldsymbol{A}+\boldsymbol{I} \\ \boldsymbol{X} \in \mathbb{R}^{n \times d} &:= \text{The input data (i.e., the per-node feature vectors)} \\ \boldsymbol{W}_1 \in \mathbb{R}^{d \times d_1} &:= \text{The neural network's first layer's weights} \\ &= \sigma(.) := The activation function (e.g., ReLU)\end{align*}$$

When I first saw this equation I found it to be quite confusing. Here is what each matrix multiplication is doing in this function:


The first three matrices are normalizing the adjacency matrix. To simplify the equation, we can let

$$\tilde{\boldsymbol{A}} := \hat{\boldsymbol{D}}^{-1/2}(\boldsymbol{A}+\boldsymbol{I})\hat{\boldsymbol{D}}^{-1/2}$$

Then, the graph convolutional layer function becomes:

$$\boldsymbol{H}_1 := \sigma\left(\tilde{\boldsymbol{A}}\boldsymbol{X}\boldsymbol{W}_1\right)$$


<center><img src="https://raw.githubusercontent.com/mbernste/mbernste.github.io/master/images/GCN_aggregation_matrices.png" alt="drawing" width="600"/></center>

<br>

<center><img src="https://raw.githubusercontent.com/mbernste/mbernste.github.io/master/images/GCN_as_neural_net.png" alt="drawing" width="800"/></center>


Related links
-------------

* **A Gentle Introduction to Graph Neural Networks** [https://distill.pub/2021/gnn-intro/](https://distill.pub/2021/gnn-intro/)
* **Graph Convolutional Networks** [https://distill.pub/2021/gnn-intro/](https://distill.pub/2021/gnn-intro/)






