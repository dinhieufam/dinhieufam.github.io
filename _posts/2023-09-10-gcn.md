---
title: 'Graph convolutional neural networks'
date: 2023-09-10
permalink: /posts/gcn/
tags:
  - tutorial
  - deep learning
  - machine learning
---

_This post is currently under construction_ 

Introduction
------------

Graphs are ubiqitous mathematical objects that describe a set of relationships between entities; however, they are challenging to model with traditional machine learning methods, which require that the input be represented as a tensor. Graphs break this paradigm due to the fact that the order of edges and nodes are arbitrary -- that is, they are _permutation invariant_ -- and the model must be capable of accomodating this feature.  In this post, we will discuss graph convolutional networks (GCNs) as presented by [Kipf and Welling, 2016](https://arxiv.org/abs/1609.02907): a class of neural network designed to operate on graphs. As their name suggestions, graph convolutional neural networks can be understood as performing a convolution in the same way that traditional convolutional neural networks (CNNs) are performing a convolution-like operation (i.e., [cross correlation](https://en.wikipedia.org/wiki/Cross-correlation)) when operating on image data. This analogy is depicted below:

<br>

<center><img src="https://raw.githubusercontent.com/mbernste/mbernste.github.io/master/images/GCN_vs_CNN_overview.png" alt="drawing" width="500"/></center>

In this post, we will discuss the intution behind the GCN and how it is similar and different to the CNN. We will conclude by presenting a case-study training a GCN to classify molecule toxicity. 

Inputs and outputs to a GCN
---------------------------


The graph convolutional layer
-----------------------------

Let $\boldsymbol{X} \in \mathbb{R}^{n \times d}$ be the features corresponding to the nodes where $n$ is the number and $d$ is the number of features. Then, the network's first layer's "features", denoted $\boldsymbol{H}_1 \in \mathbb{R}^{n \times d_1}$, where $d_1$ is the number of features in the first layer, is computed as:

$$\boldsymbol{H}_1 := \sigma\left(\hat{\boldsymbol{D}}^{-1/2}(\boldsymbol{A}+\boldsymbol{I})\hat{\boldsymbol{D}}^{-1/2} \boldsymbol{X}\boldsymbol{W}_1\right)$$

where,

$$\begin{align*}$\boldsymbol{A} \in \mathbb{R}^{n \times n} &:= \text{The adjacency matrix} \\ \boldsymbol{I} \in \mathbb{R}^{n \times n} &:= \text{The identity matrix} \\ \hat{\boldsymbol{D}} \in \mathbb{R}^{n \times n} &:= \text{The degree matrix of } \ \boldsymbol{A}+\boldsymbol{I} \\ \boldsymbol{W}_1 &:= \text{The neural network's first layer's weights}\end{align*}$$

When I first saw this equation I found it to be quite confusing. Here is what each matrix multiplication is doing in this function:


The first three matrices are normalizing the adjacency matrix. To simplify the equation, we can let

$$\tilde{\boldsymbol{A}} := \hat{\boldsymbol{D}}^{-1/2}(\boldsymbol{A}+\boldsymbol{I})\hat{\boldsymbol{D}}^{-1/2}$$

Then, the graph convolutional layer function becomes:

$$\boldsymbol{H}_1 := \sigma\left(\tilde{\boldsymbol{A}}\boldsymbol{X}\boldsymbol{W}_1\right)$$


<center><img src="https://raw.githubusercontent.com/mbernste/mbernste.github.io/master/images/GCN_aggregation_matrices.png" alt="drawing" width="600"/></center>

<br>

<center><img src="https://raw.githubusercontent.com/mbernste/mbernste.github.io/master/images/GCN_as_neural_net.png" alt="drawing" width="800"/></center>


Normalizing the adjacency matrix
--------------------------------






