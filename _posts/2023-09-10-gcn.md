---
title: 'Graph convolutional neural networks'
date: 2023-09-10
permalink: /posts/gcn/
tags:
  - tutorial
  - deep learning
  - machine learning
---

_This post is currently under construction_ 

Introduction
------------

Graphs are ubiqitous mathematical objects that describe a set of relationships between entities; however, they are challenging to model with traditional machine learning methods, which require that the input be represented as a tensor. Graphs break this paradigm due to the fact that the order of edges and nodes are arbitrary -- that is, they are _permutation invariant_ -- and the model must be capable of accomodating this feature.  In this post, we will discuss graph convolutional networks (GCNs) as presented by [Kipf and Welling, 2016](https://arxiv.org/abs/1609.02907): a class of neural network designed to operate on graphs. As their name suggestions, graph convolutional neural networks can be understood as performing a convolution in the same way that traditional convolutional neural networks (CNNs) are performing a convolution-like operation (i.e., [cross correlation](https://en.wikipedia.org/wiki/Cross-correlation)) when operating on image data. This analogy is depicted below:

<br>

<center><img src="https://raw.githubusercontent.com/mbernste/mbernste.github.io/master/images/GCN_vs_CNN_overview.png" alt="drawing" width="500"/></center>

In this post, we will discuss the intution behind the GCN and how it is similar and different to the CNN. We will conclude by presenting a case-study training a GCN to classify molecule toxicity. 

Inputs and outputs to a GCN
---------------------------


The graph convolutional layer
-----------------------------

Let $\boldsymbol{X} \in \mathbb{R}^{n \times d}$ be the features corresponding to the nodes where $n$ is the number and $d$ is the number of features. Then, the network's first layer's "features", denoted $\boldsymbol{H}_1 \in \mathbb{R}^{n \times d_1}$, where $d_1$ is the number of features in the first layer, is computed as:

$$\boldsymbol{H}_1 := \sigma\left(\tilde{\boldsymbol{A}}\boldsymbol{X}\boldsymbol{W}_1\right)$$

where $\tilde{\boldsymbol{A}}$ is the normalized adjacency matrix and $\boldsymbol{W}_1$ are the neural network's first layer's weights. When I first saw this equation I found it to be quite confusing. Let's break it down. The first [matrix multiplication]([https://mbernste.github.io/posts/matrix_vector_mult/](https://mbernste.github.io/posts/matrix_multiplication/)), $\tilde{\boldsymbol{A}}\boldsymbol{X}$ can be interpreted as an aggregation step where we take the mean of the features of all adjacent nodes:

<center><img src="https://raw.githubusercontent.com/mbernste/mbernste.github.io/master/images/GCN_aggregation_matrices.png" alt="drawing" width="600"/></center>


Normalizing the adjacency matrix
--------------------------------




<br>

<center><img src="https://raw.githubusercontent.com/mbernste/mbernste.github.io/master/images/GCN_as_neural_net.png" alt="drawing" width="800"/></center>


