

Introduction
------------

The advancement of technology has brought with it the ability to generate ever larger and more complex collections of data. This is especially true in biomedical research, where new technologies can produce thousands, or even millions, of biomolecular measurements at a time. Because we human beings use our vision as our chief sense for understanding the world, when we are confronted with data, we try to understand that data through visualization. Moreover, because we evolved in a three-dimensional world, we can only ever visualize up to three dimensions of an object at a time. This limitation poses a fundamental problem when it comes to high-dimensional data; high-dimensional data cannot, without loss of information, be visualized in their totality at once. But this does not mean we have not tried! The field of [dimension reduction algorithms](https://en.wikipedia.org/wiki/Dimensionality_reduction#:~:text=Dimensionality%20reduction%2C%20or%20dimension%20reduction,close%20to%20its%20intrinsic%20dimension.) studies and develops algorithms that map high dimensional data to two or three dimensions where we can visualize it with minimal loss of information. For example, the classical [principal components analysis (PCA)](https://en.wikipedia.org/wiki/Principal_component_analysis) uses a linear mapping to project data down to a space that preserves as much variance as possible. More recently, the [t-SNE](https://www.jmlr.org/papers/volume9/vandermaaten08a/vandermaaten08a.pdf) and [UMAP](https://arxiv.org/pdf/1802.03426.pdf) algorithms use [nonlinear mappings](https://en.wikipedia.org/wiki/Nonlinear_dimensionality_reduction) that attempts to preserve the "topology" of the data -- that is, that attempts to preserve neighborhoods of nearby data points while preventing overlapping dense regions of data in the output figure. A few examples of [single-cell RNA-seq](https://en.wikipedia.org/wiki/Single-cell_sequencing) data visualized with these three approaches are shown below:



Unfortunately, because it is mathematically impossible to avoid losing information when mapping data from high to low dimensions, these algorithms inevitibly lose some aspect of the data, either by distortion or ommision, when plotting it in lower dimensions. This limitation makes the figures generated by these methods easy to misinterpret. Because of this, dimension reduction algorithms, especially [t-SNE](https://www.jmlr.org/papers/volume9/vandermaaten08a/vandermaaten08a.pdf) and [UMAP](https://arxiv.org/pdf/1802.03426.pdf), are facing [new scrutiny](https://journals.plos.org/ploscompbiol/article?id=10.1371/journal.pcbi.1011288) by those who argue that nonlinear dimension reduction algorithms distort the data so heavily that their output is at best useless and at worst harmful. On the other hand, proponents of these methods argue that distortion is inevitable, but that these methods can and do reveal aspects of the data's structure.

In this blog post, I will attempt to provide my views on the matter that balance the views of the critics and proponents. I will start with a review of what it means to "visualize" data. I will also review what it means to perform dimensionality reduction. Finally, I will argue that dimension reduction methods require a different kind of mentality when using them than traditional data visualizations that do not attempt to compress high dimensional data into few dimensions. I will also argue that human user-studies of these methods are required to evaluate their utility.

For much of this blog, I will use data generated by single-cell [RNA-sequencing](https://mbernste.github.io/posts/rna_seq_basics/) (scRNA-seq) as the primary example of high-dimensional data which I will use in a case study addressing the risks and merits of using dimension reduction for data visualization. For a comprehensive review on RNA-seq, please see [my previous blog post](https://mbernste.github.io/posts/rna_seq_basics/). 

Data visualization: mapping numbers to sizes, distances, and colors
-------------------------------------------------------------------

Scatterplots as a method for visualizing structure
--------------------------------------------------

Dimensionality reduction entails a loss of information
------------------------------------------------------

Before moving forward, let's formalize what we mean by the "dimensionality" of data. For the purposes of our discussion, we will refer to data as being "n$ dimensionality if that data can be represented as a set of coordinate vectors in $\mathbb{R}^d$. That is, the dataset can be represented as $\boldsymbol{x}_1, \dots \boldsymbol{x}_n \in \mathbb{R}^d$. Collectivly, we can represent the data as a matrix $\boldsymbol{X}^{n \times d}$ where each row represents a datapoint. This description thus covers all tabular data. 

The task of dimensionality reduction is to find a function $f$ that maps our vectors in $\mathbb{R}^d$ to a new space $\mathbb{R}^{d'}$ where $d' < d$:

$$f : \mathbb{R}^d \rightarrow \mathbb{R}^{d'}$$

For visualization purposes, if $d > 3$ we cannot easily visualize our data as a scatterplot to see the global structure between datapoints. Thus, it is common to set $d'$ to either 2 or 3 thus mapping each datapoint $\boldsymbol{x}_i$ to a new, 2 or 3 dimensional data point $x'_i$ that can be visualized in a scatterplot. That is,

$$\boldsymbol{x}'_i := f(\boldsymbol{x})$$

&nbsp;

<center><img src="https://raw.githubusercontent.com/mbernste/mbernste.github.io/master/images/globe_projections_by_Daniel_R_Strebe.png" alt="drawing" width="600"/></center>

&nbsp;


Inferences are the primary product of a data visualization
----------------------------------------------------------

A probabistic framework for thinking about inferences generated by data visualizations
--------------------------------------------------------------------------------------




