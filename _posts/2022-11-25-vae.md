---
title: 'Variational autoencoders'
date: 2022-11-25
permalink: /posts/vae/
tags:
  - tutorial
  - deep learning
  - machine learning
  - probabilistic models
---

_THIS POST IS CURRENTLY UNDER CONSTRUCTION_

Introduction
------------

Variational autoencoders (VAEs), introduced by [Kingma and Welling (2013)](https://arxiv.org/abs/1312.6114_) are a class of probabilistic models that find latent, low-dimensional representations of data. VAEs are thus a method for performing [dimensionality reduction](https://en.wikipedia.org/wiki/Dimensionality_reduction) to reduce data down to their [intrinsic dimensionality](https://mbernste.github.io/posts/intrinsic_dimensionality/). 

The name "variational autoencoder" implies that these models are a type of [autoencoder](https://en.wikipedia.org/wiki/Autoencoder). That is, they are models that take a vector, $\boldsymbol{x}$, compress it into a lower-dimensional vector, $\boldsymbol{z}$, and then decompress $\boldsymbol{z}$ back into $\boldsymbol{x}$. The architecture of an autoencoder can can be visualized as follows:

&nbsp;

<center><img src="https://raw.githubusercontent.com/mbernste/mbernste.github.io/master/images/autoencoder.png" alt="drawing" width="350"/></center>

&nbsp;

Here we see one function (usually a neural network), $h_\phi$, compresses $\boldsymbol{x}$ into a low-dimensional data point, $\boldsymbol{z}$, and then another function (also a neural network), $f_\theta$, decompresses it back into an approximation of $\boldsymbol{x}$, here denoted as $\boldsymbol{x}'$. The variables $\phi$ and $\theta$ denote the parameters to the two neural networks.  

VAEs can be viewed as a type of autoencoder like the one shown above with a few key differences. At their foundation, VAEs are probablistic models and as we will see in this post, their "autoencoding" function arises indirectly from the way in which we define the generative model and the [variational inference](https://mbernste.github.io/posts/variational_inference/)-based procedure used to fit that model. 

I think it helps to view VAEs [from two angles](https://mbernste.github.io/posts/understanding_3d/):

1. **Probabilistic generative models:** VAEs are probabilistic generative models of independent, identically distributed samples, $\boldsymbol{x}_1, \dots, \boldsymbol{x}_m$. In this model, each sample, $\boldsymbol{x}_i$, is associated with a latent (i.e. unobserved), lower-dimensional variable $\boldsymbol{z}_i$. Variational autoencoders are a generative model in that they describe a joint distribution over samples and their associated latent variable, $p(\boldsymbol{x}, \boldsymbol{z})$. 
2. **Autoencoder:** VAEs are a form of autoencoders. Unlike traditional autoencoders, VAEs can be veiwed as _probabilistic_ rather than deterministic; Given an input sample, $\boldsymbol{x}\_i$, the compressed representation of $\boldsymbol{x}\_i$, $\boldsymbol{z}\_i$, is randomly generated.

In this blog post we will show how VAEs can be viewed through both of these lenses. We will then present an example of running VAEs on MNIST. 

VAEs as probabilistic generative models
---------------------------------------

At their foundation, VAEs define probabilistic generative process for "generating" data points in some $J$-dimensional vector space. This generative process goes as follows: we first sample a latent variable $\boldsymbol{z} \in \mathbb{R}^{D}$, where $D < J$ from some distribution, and then use a determinstic function to map $\boldsymbol{z}$ to $\boldsymbol{x}$. Typically, $\boldsymbol{z}$ is made to follow a standard normal distribution:

$$\boldsymbol{z} \sim N(\boldsymbol{0}, \boldsymbol{I})$$

Then, we use $\boldsymbol{z}$ to construct the parameters, $\boldsymbol{\psi}$, of another distribution used to sample $\boldsymbol{x}$. Most commonly, we construct $\psi$ from $\boldsymbol{z}$ using neural networks:

$$\begin{align*} \boldsymbol{\psi} &:= f_{\theta}(\boldsymbol{z}) \\ \boldsymbol{x} &\sim \mathcal{D}(\boldsymbol{\psi}) \end{align*}$$

where $\mathcal{D}$ is a parametric distribution and $f$ is a neural network parameterized by a set of parameters $\theta$. As one example, $\mathcal{D}$ may be a Gaussian distribution with unit variance and $\psi$ is the mean of this distribution.  Here's a schematic illustration of the generative process:

&nbsp;

<center><img src="https://raw.githubusercontent.com/mbernste/mbernste.github.io/master/images/VAE_generative_process_shapes.png" alt="drawing" width="700"/></center>

&nbsp;

This generative process can be visualized graphically below:

&nbsp;

<center><img src="https://raw.githubusercontent.com/mbernste/mbernste.github.io/master/images/VAE_generative_process.png" alt="drawing" width="700"/></center>

&nbsp;

Interestingly, this model enables use to fit very complicated distributions! That's because although the distribution over $\boldsymbol{z}$ and the conditional distribution of $\boldsymbol{x}$ given $\boldsymbol{z}$ may be simple (e.g., both simply normal distributions), the marginal distribution of $\boldsymbol{x}$ becomes complex:

&nbsp;

<center><img src="https://raw.githubusercontent.com/mbernste/mbernste.github.io/master/images/VAE_marginal.png" alt="drawing" width="350"/></center>

&nbsp;

This complexity is a result of the non-linear mapping between $\boldsymbol{z}$ and $\psi$ implemented via the neural network!

### Inference

Now, let's say we are given a dataset consisting of data points $\boldsymbol{x}_1, \dots, \boldsymbol{x}_m \in \mathbb{R}^J$ that we assume was generated by a VAE. We may be interested in two central tasks:
1. For fixed $\theta$, for each $\boldsymbol{x}\_i$, compute the posterior distribution $p_{\theta}(\boldsymbol{z}_i \mid \boldsymbol{x}_i)$
2. Find the maximum likelihood estimates of $\theta$

Unfortunately, for a fixed $\theta$, solving for the posterior $p_{\theta}(\boldsymbol{z}_i \mid \boldsymbol{x}_i)$ using Bayes Theorem is intractible due the denominator in the formula for Bayes Theorem requires marginalizing over $\boldsymbol{z}_i$:

$$p_\theta(\boldsymbol{z}_i \mid \boldsymbol{x}_i) = \frac{p_\theta(\boldsymbol{x}_i \mid \boldsymbol{z}_i)p(\boldsymbol{z}_i)}{\int p_\theta(\boldsymbol{x}_i, \boldsymbol{z}_i) \ d\boldsymbol{z}_i }$$

Similarly, it is difficult to estimate the values for $\theta$ by maximizing the data likelihood due to the need to marginalize over each $\boldsymbol{z}\_i$:

$$\begin{align*}\hat{\theta} &:= \text{argmax}_\theta \prod_{i=1}^m p_\theta(\boldsymbol{x}_i) \\ &= \text{argmax}_\theta \prod_{i=1}^m \int p_\theta(\boldsymbol{x}_i, \boldsymbol{z}_i) \ d\boldsymbol{z}_i  \end{align*}$$

Variatonal autoencoders find approximate solutions to these intractible inference problems using [variational inference](https://mbernste.github.io/posts/variational_inference/). 

First, let's assume that $\theta$ is fixed and attempt to approximate $p\_\theta(\boldsymbol{z}\_i \mid \boldsymbol{x}\_i)$. Variational inference is a method for performing such approximations by first choosing a set of probability distributions, $\mathcal{Q}$, called the _variational family_, and then finding the distribution $q(\boldsymbol{z}\_i) \in \mathcal{Q}$ that is "closest to" $p\_\theta(\boldsymbol{z}\_i \mid \boldsymbol{x}\_i)$. Variational inference uses the [KL-divergence](https://en.wikipedia.org/wiki/Kullback%E2%80%93Leibler_divergence) between $q(\boldsymbol{z}\_i)$ and $p_\theta(\boldsymbol{z}\_i \mid \boldsymbol{x}\_i)$ as its measure of "closeness". Thus, the goal of variational inference is to minimize the KL-divergence. It turns out that the task of minimizing the KL-divergence is equivalent to the task of maximizing a quantity called the [evidence lower bound (ELBO)](https://mbernste.github.io/posts/elbo/), which is defined as

$$\begin{align*} \text{ELBO}(q) &:= E_{\boldsymbol{z}_1, \dots, \boldsymbol{z}_m  \overset{\text{i.i.d.}}{\sim} q}\left[ \sum_{i=1}^m \log p_\theta(\boldsymbol{x}_i, \boldsymbol{z}_i) - \sum_{i=1}^m \log q(\boldsymbol{z}_i) \right] \\ &= \sum_{i=1}^m E_{z_i \sim q} \left[\log p_\theta(\boldsymbol{x}_i, \boldsymbol{z}_i) - \log q(\boldsymbol{z}_i) \right] \end{align*}$$

Thus, variational inference entails finding

$$\hat{q} := \text{arg max}_{q \in \mathcal{Q}} \ \text{ELBO}(q)$$

Now, so far we have assumed that $\theta$ is fixed. Is it possible to find both $q$ and $\theta$ jointly? As we discuss in a [previous post on variational inference](https://mbernste.github.io/posts/reparameterization_vi/), it is perfectly reasonable to define the ELBO as a function of _both_ $q$ and $\theta$ and then to maximize the ELBO jointly with respect to both of these parameters:

$$\hat{q}, \hat{\theta} := \text{arg max}_{q, \theta} \ \text{ELBO}(q, \theta)$$

Why is this a reasonable thing to do? Recall the ELBO is a _lower bound_ on the marginal log-likelihood $p_\theta(x_1, \dots, x_m)$. Thus, optimizing the ELBO with respect to $\theta$ increases the lower bound of the log-likelihood.


### Variational family used by VAEs

VAEs use a variational family with the following form:

$$\mathcal{Q} := \left\{ N(h^{(1)}_\phi(\boldsymbol{x}), \exp(h^{(2)}\phi(\boldsymbol{x})) \boldsymbol{I}) \mid \phi \in \mathbb{R}^R \right\}$$

where $h^{(1)}\_\phi$ and $h^{(1)}\_\phi$ are two neural networks that map the original object, $\boldsymbol{x}$, to the mean, $\boldsymbol{\mu}$, and the logarithm of the variance, $\log \boldsymbol{\sigma}^2$, of the approximate posterior respectively. $R$ is the number of parameters to these neural networks. 

Said a different way, we define $q\_\phi(\boldsymbol{z} \mid \boldsymbol{x})$ as

$$q_\phi(\boldsymbol{z} \mid \boldsymbol{x}) := N(h^{(1)}_\phi(\boldsymbol{x}), \exp(h^{(2)}_\phi(\boldsymbol{x})) \boldsymbol{I})$$

Said a third way, the approximate posterior distribution can be sampled via the following process:

$$\begin{align*}\boldsymbol{\mu} &:=  h^{(1)}_\phi(\boldsymbol{x}) \\ \log \boldsymbol{\sigma}^2 &:= h^{(2)}_\phi(\boldsymbol{x}) \\ \boldsymbol{z} &\sim N(\boldsymbol{\mu}, \boldsymbol{\sigma^2}\boldsymbol{I}) \end{align*}$$

This can be visualized as follows:

&nbsp;

<center><img src="https://raw.githubusercontent.com/mbernste/mbernste.github.io/master/images/VAE_encoder.png" alt="drawing" width="700"/></center>

&nbsp;

Note that $h^{(1)}\_\phi$ and $h^{(2)}\_\phi$ may either be two entirely separate neural networks or may share some subset of parameters. We use $h_\phi$ to refer to the full neural network (or union of two separate neural networks) comprising both $h^{(1)}\_\phi$ and $h^{(2)}\_\phi$ as shown below:

&nbsp;

<center><img src="https://raw.githubusercontent.com/mbernste/mbernste.github.io/master/images/VAE_encoder_architecture.png" alt="drawing" width="500"/></center>

&nbsp;


Thus, maximizing the ELBO over $\mathcal{Q}$ reduces to maximizing the ELBO over the neural network parameters $\phi$ (in addition to $\theta$ as discussed previously):

$$\begin{align*}\hat{\phi}, \hat{\theta} &= \text{arg max}_{\phi, \theta} \  \text{ELBO}(\phi, \theta) \\ &:= \text{arg max}_{\phi, \theta} \  \sum_{i=1}^m E_{\boldsymbol{z}_i \sim q_\phi(\boldsymbol{z}_i \mid \boldsymbol{x}_i)}\left[ \log p_\theta(\boldsymbol{x}_i, \boldsymbol{z}_i) - \log q_\phi(\boldsymbol{z}_i \mid \boldsymbol{x}_i) \right] \end{align*}$$

One detail to point out here is that the approximation of the posterior over each $\boldsymbol{z}\_i$ is defined by a set of parameters $\phi$ that are shared accross all samples $\boldsymbol{z}\_1, \dots, \boldsymbol{z}\_m$. That is, we use a single set of neural network parameters $\phi$ to encode the posterior distribution $q\_\phi(\boldsymbol{z}\_i \mid \boldsymbol{x}\_i)$. Note, we _could_ have gone a different route and defined a _separate_ variational distribution $q\_i$ for each $\boldsymbol{z}\_i$ that is not conditioned on $\boldsymbol{x}\_i$. That is, to define the variational posterior as $q\_{\phi_i}(\boldsymbol{z}\_i)$, where each $\boldsymbol{z}\_i$ has its own set of parameters $\phi\_i$. Here, $q\_{\phi\_i}(\boldsymbol{z}\_i)$ does not condition on $\boldsymbol{x}\_i$. Why don't we do this instead? The answer is that for extremely large datasets it's easier to perform VI when $\phi$ are shared across all data points because it reduces the number of parameters we need to search over in our optimization. This act of defining a common set of parameters shared across all of the independent posteriors is called **amortized variational inference**.

### Maximizing the ELBO

Now that we've set up the optimization problem, we need to solve it. Unfortunately, the expecation present in the ELBO makes this difficult as it requires integrating over all possible values for $\boldsymbol{z}_i$:

$$\begin{align*}\text{ELBO}(\phi, \theta) &= \sum_{i=1}^m E_{\boldsymbol{z} \sim q_\phi(\boldsymbol{z} \mid \boldsymbol{x})}\left[ \log p_\theta(\boldsymbol{x}_i, \boldsymbol{z}_i) - \log q_\phi(\boldsymbol{z}_i \mid \boldsymbol{x}_i) \right] \\ &= \sum_{i=1}^m \int_{\boldsymbol{z}_i}   q_\phi(\boldsymbol{z}_i \mid \boldsymbol{x}_i) \left[ \log p_\theta(\boldsymbol{x}_i, \boldsymbol{z}_i) - \log q_\phi(\boldsymbol{z}_i \mid \boldsymbol{x}_i) \right] \ d\boldsymbol{z}_i \end{align*}$$

We address this challenge by using the **reparameterization gradient** method, which we discussed in a [previous blog post](https://mbernste.github.io/posts/reparameterization_vi/). We will review this method here; however see my previous post for a more detailed explanation. 

In brief, the reparameterization method maximizes the ELBO via stochastic gradient ascent in which stochastic gradients are formulated by first performing the **reparameterization trick** followed by Monte Carlo sampling.  The reparameterization trick works as follows: we "reparameterize" the distribution $q_\phi(z_i \mid x_i)$ in terms of a surrogate random variable $\epsilon_i \sim \mathcal{D}$ and a determinstic function $g$ in such a way that sampling $z$ from $q_\phi(z_i \mid x_i)$ is performed as follows:

$$\begin{align*}\epsilon_i &\sim \mathcal{D} \\ z_i &:= g_\phi(\boldsymbol{\epsilon}_i, x_i)\end{align*}$$

One way to think about this is that instead of sampling $z$ directly from our variational posterior $q_\phi(z_i \mid x_i)$, we "re-design" the generative process of $z_i$ such that we first sample a surrogate random variable $\epsilon\_i$ and then transform $\epsilon\_i$ into $z_i$ all while ensuring that in the end, the distribution of $z\_i$ still follows $q_\phi(z_i \mid x_i)$. Following the reparameterization trick, we can re-write the ELBO as follows:

$$\text{ELBO}(\phi, \theta) := \sum_{i=1}^m E_{\epsilon_i \sim \mathcal{D}}\left[ \log p_\theta(\boldsymbol{x}_i, g_\phi(\boldsymbol{\epsilon}_i, \boldsymbol{x}_i) - \log q_\phi(g_\phi(\boldsymbol{\epsilon}_i, \boldsymbol{x}_i) \mid \boldsymbol{x}_i) \right]$$

We then approximate the ELBO via Monte Carlo sampling. That is, for each sample, $i$, we first sample random variables from our surrogate distribution $\mathcal{D}$:

$$\boldsymbol{\epsilon}'_{i,1}, \dots, \boldsymbol{\epsilon}'_{i,L} \sim \mathcal{D}$$

Then we can compute a Monte Carlo approximation to the ELBO:

$$\tilde{\text{ELBO}}(\phi, \theta)  \sum_{i=1}^m \sum_{l=1}^L \left[ \log p_\theta(\boldsymbol{x}_i, g_\phi(\boldsymbol{\epsilon}'_{i,l}, \boldsymbol{x}_i)) - \log q_\phi(g_\phi(\boldsymbol{\epsilon}'_{i,l}, \boldsymbol{x}_i) \mid \boldsymbol{x}_i) \right]$$

Now the question becomes, what reparamterization can we use? Recall that for the VAEs discussed here, our $q_\phi(\boldsymbol{z} \mid \boldsymbol{x})$ is a normal distribution:

$$q_\phi(\boldsymbol{z} \mid \boldsymbol{x}) := N(h^{(1)}_\phi(\boldsymbol{x}), \exp(h^{(2)}_\phi(\boldsymbol{x})) \boldsymbol{I})$$

This naturally can be reparameterized as:

$$\begin{align*}\boldsymbol{\epsilon}_i &\sim N(\boldsymbol{0}, \boldsymbol{I}) \\ z_i &:= h^{(1)}_\phi(\boldsymbol{x}) + \exp(h^{(2)}_\phi(\boldsymbol{x}))\boldsymbol{\epsilon}_i \end{align*}$$

Thus, our function $g$ is simply the function that shifts $\boldsymbol{\epsilon}_i$ by $h^{(1)}\_\phi(\boldsymbol{x})$ and scales it by $\exp(h^{(2)}\_\phi(\boldsymbol{x}))$. That is, 

$$g(\boldsymbol{\epsilon}_i, \boldsymbol{x}_i) := h^{(1)}_\phi(\boldsymbol{x}) + \exp(h^{(2)}_\phi(\boldsymbol{x}))\boldsymbol{\epsilon}_i$$

Because $\tilde{\text{ELBO}}(\phi, \theta)$ is differentiable with respect to both $\phi$ and $\theta$ (notice that $f_\phi(\boldsymbol{x}_i)$ and $h_\phi(\boldsymbol{x}_i)$ are neural networks which are differentiable), we can form the gradient:

$$\nabla_{\phi, \theta} \tilde{\text{ELBO}}(\phi, \theta) = \sum_{i=1}^m \sum_{l=1}^L \nabla_{\phi, \theta} \left[ \log p_\theta(\boldsymbol{x}_i, g_\phi(\epsilon'_{i,l}, \boldsymbol{x}_i) - \log q_\phi(g_\phi(\epsilon'_{i,l}, \boldsymbol{x}_i) \mid \boldsymbol{x}_i) \right]$$

This gradient can then be used to perform gradient ascent. To compute this gradient, we can apply [automatic differentiation](https://en.wikipedia.org/wiki/Automatic_differentiation). Then we an use these gradients to perform [gradient descent](https://en.wikipedia.org/wiki/Gradient_descent)-based optimization. Thus, we can utilize the extensive toolkit developed for training deep learning models! 

### Reducing the variance of the stochastic gradients  

For the VAE model there is a modification that we can make to reduce the variance of the Monte Carlo gradients. We first re-write the original ELBO in a different form:

$$\begin{align*}\text{ELBO}(\phi, \theta) &= \sum_{i=1}^m E_{z_i \sim q} \left[ \log p_\theta(\boldsymbol{x}_i, \boldsymbol{z}_i) - \log q( \boldsymbol{z}_i \mid \boldsymbol{x}_i) \right] \\ &= \sum_{i=1}^m \int q(\boldsymbol{z}_i \mid \boldsymbol{x}_i) \left[\log p_\theta(\boldsymbol{x}_i, \boldsymbol{z}_i) - \log q(\boldsymbol{z}_i \mid \boldsymbol{x}_i) \right] \ d\boldsymbol{z}_i \\ &= \sum_{i=1}^m \int q(\boldsymbol{z}_i \mid \boldsymbol{x}_i) \left[\log p_\theta(\boldsymbol{x}_i \mid \boldsymbol{z}_i) + \log p(\boldsymbol{z}_i) - \log q(\boldsymbol{z}_i \mid \boldsymbol{x}_i) \right] \ d\boldsymbol{z}_i \\ &= \sum_{i=1}^m E_{\boldsymbol{z}_i \sim q} \left[ \log p_\theta(\boldsymbol{x}_i \mid \boldsymbol{z}_i) \right] + \sum_{i=1}^m \int q(\boldsymbol{z}_i \mid \boldsymbol{x}_i) \left[\log p(\boldsymbol{z}_i) - \log q(\boldsymbol{z}_i \mid \boldsymbol{x}_i) \right] \ d\boldsymbol{z}_i \\ &= \sum_{i=1}^m E_{\boldsymbol{z}_i \sim q} \left[ \log p_\theta(\boldsymbol{x}_i \mid \boldsymbol{z}_i) \right] - \sum_{i=1}^m E_{\boldsymbol{z}_i \sim q}\left[ \int q(\boldsymbol{z}_i \mid \boldsymbol{x}_i)\log \frac{ p(\boldsymbol{z}_i)}{q(\boldsymbol{z}_i \mid \boldsymbol{x}_i)} \right] \\ &= \sum_{i=1}^m E_{\boldsymbol{z}_i \sim q} \left[ \log p_\theta(\boldsymbol{x}_i \mid \boldsymbol{z}_i) \right] - KL(q(\boldsymbol{z}_i \mid \boldsymbol{x}_i) \ || \ p(\boldsymbol{z}_i)) \end{align*}$$ 

Recall the VAEs we have considered in this blog post have defined $p(\boldsymbol{z})$ to be the standard normal distribution $N(\boldsymbol{0}, \boldsymbol{I})$. In this particular case, it turns out that the KL-divergence term above can be expressed analytically (See the Appendix to this post):

$$KL(q_\phi(\boldsymbol{z}_i \mid \boldsymbol{x}_i) \mid\mid p(\boldsymbol{z})) = -\frac{1}{2} \sum_{j=1}^J \left(1 + h^{(2)}_\phi(\boldsymbol{x}_i)_j - \left(h^{(1)}_\phi(\boldsymbol{x}_i)\right)_j^2 - \exp(h^{2}_\phi(\boldsymbol{x}_i)_j) \right)$$

Note above the KL-divergence is calculated by summing over each dimension in the latent space. The full ELBO is:

$$\begin{align*} \text{ELBO}(\phi, \theta) &= \sum_{i=1}^m \left[\frac{1}{2} \sum_{j=1}^J \left(1 + h^{(2)}_\phi(\boldsymbol{x}_i)_j - \left(h^{(1)}_\phi(\boldsymbol{x}_i)\right)_j^2 - \exp(h^{2}_\phi(\boldsymbol{x}_i)_j) \right) + E_{\boldsymbol{z}_i \sim q_\phi(\boldsymbol{z}_i \mid \boldsymbol{x}_i)} \left[\log p_\theta(\boldsymbol{x}_i \mid \boldsymbol{z}_i) \right]\right] \end{align*}$$

Then, we can apply the reparameterization trick to this formulation of the ELBO and derive the following Monte Carlo approximation:

$$\begin{align*}\text{ELBO}(\phi, \theta) &\approx \sum_{i=1}^m \left[\frac{1}{2} \sum_{j=1}^J \left(1 + h^{(2)}_\phi(\boldsymbol{x}_i)_j - \left(h^{(1)}_\phi(\boldsymbol{x}_i)\right)_j^2 - \exp(h^{2}_\phi(\boldsymbol{x}_i)_j) \right) + \frac{1}{L} \sum_{l=1}^L \left[\log p_\theta(\boldsymbol{x}_i \mid g_\phi(\boldsymbol{x}_{i}) + h_\phi(\boldsymbol{x}_i)\epsilon_{i,l}) \right] \right]\end{align*}$$

Though this equation looks daunting, the feature to notice is that it is differentiable with respect to $\phi$ and $\theta$. Thus, we can then apply automatic differentation to derive the gradients with respect to $\phi$ and $\theta$ that are needed to perform stochastic gradient ascent!

Lastly, one may ask: why does this stochastic gradient have reduced variance than the version discussed previously? Intuitively, terms within the ELBO's expectation are being "pulled out" and computed analytically (i.e., the KL-divergence). Since these terms are analytical, less of this quantity is determined by the variability from sampling each $\boldsymbol{\epsilon}'_{i,l}$ and thus, there will be less overall variability.

VAEs as autoencoders
---------------------

So far, we have described VAEs in the context of probabilistic modeling. That is, we have described how the VAE is a probabilistic model that describes each high-dimensional datapoint, $\boldsymbol{x}_i$, as being "generated" from a lower dimensional data point $\boldsymbol{z}_i$. This generating procedure utilizes a neural network to map  $\boldsymbol{z}_i$ to the parameters of the distribution $\mathcal{D}$ required to sample $\boldsymbol{x}_i$. Moreover, we can infer the parameters and latent variables to this model via VI. To do so, we solve a sort of inverse problem in which use a neural network to map each $\boldsymbol{x}_i$ into parameters of the variational posterior distribution $q$ required to sample $\boldsymbol{z}_i$. 

Now, what happens if we tie the variational posterior $q_\phi(\boldsymbol{z} \mid \boldsymbol{x})$ to the data generating distribution $p_\theta(\boldsymbol{x} \mid \boldsymbol{z})$? That is, given a data point $\boldsymbol{x}$, we first sample $\boldsymbol{z}$ from the variational posterior distribution,

$$\boldsymbol{z} \sim q_\phi(\boldsymbol{z} \mid \boldsymbol{x})$$

then we generate a new data point, $\boldsymbol{x}'$, from $p(\boldsymbol{x} \mid \boldsymbol{z})$:

$$\boldsymbol{x}' \sim p_\theta(\boldsymbol{x} \mid \boldsymbol{z})$$

We can visualize this process schematically below: 

&nbsp;

<center><img src="https://raw.githubusercontent.com/mbernste/mbernste.github.io/master/images/VAE_as_autoencoder.png" alt="drawing" width="800"/></center>

&nbsp;

Notice the similarity of the above process to the standard autoencoder:

&nbsp;

<center><img src="https://raw.githubusercontent.com/mbernste/mbernste.github.io/master/images/autoencoder.png" alt="drawing" width="350"/></center>

&nbsp;

Recall that the goal of autoencoders is to learn a function for compressing each $\boldsymbol{x}\_i$ into a low dimensional "representation" $\boldsymbol{z}\_i$ such that we can recover $\boldsymbol{x}\_i$ from this compressed representation.  Although the loss function for the VAE is different, it performs the same sort of compression and decompression as a standard autoencoder! One can view the VAE as a "probabilistic" autoencoder. Instead of mapping each $\boldsymbol{x}\_i$ directly to $\boldsymbol{z}\_i$, the VAE maps $\boldsymbol{x}\_i$ to a _distribution_ over $\boldsymbol{z}\_i$ from which $\boldsymbol{z}\_i$ can be _sampled_. This randomly sampled $\boldsymbol{z}\_i$ is then to a distribution over $\boldsymbol{x}\_i$ which can also then be sampled.

### Viewing the VAE loss function as regularized reconstruction loss

Let's take a loser look at the loss function for the VAE with our new view of VAEs as a probabilistic autoencoder. The (exact) loss function is the negative of the ELBO:

$$\begin{align*} \text{loss}_{\text{VAE}}(\phi, \theta) &= -\sum_{i=1}^m E_{\boldsymbol{z}_i \sim q_\phi(\boldsymbol{z}_i \mid \boldsymbol{x}_i)} \left[\log p_\theta(\boldsymbol{x}_i \mid \boldsymbol{z}_i) \right] - KL(q_\phi(\boldsymbol{z}_i \mid \boldsymbol{x}_i) \ || \ p(\boldsymbol{z}_i) \end{align*}$$

Notice there are two terms with opposite signs. The first term, $\log p\_\theta(\boldsymbol{x}\_i \mid \boldsymbol{z}\_i)$, can be seen as a  **reconstruction loss** because it will push the model towards reconstructing the original $\boldsymbol{x}\_i$ from the its "compressed" representation, $\boldsymbol{z}\_i$. 

This can be made especially evident if our model assumes that $p\_\theta(\boldsymbol{x}\_i \mid \boldsymbol{z}\_i)$ is a normal distribution. That is,

$$p\_\theta(\boldsymbol{x}\_i \mid \boldsymbol{z}\_i) := N(f_\theta(\boldsymbol{z}), \sigma_{\text{decoder}}\boldsymbol{I})$$

where $\sigma_{\text{decoder}}$ describes the amount of Gaussian around $f_\theta(\boldsymbol{z})$. In this scenario, the VAE will attempt to minimize the squared error between the decoded $\boldsymbol{x}'_i$ and the original input data point $\boldsymbol{x}_i$. We can see this, by writing out, explicitly, the term $\log p\_\theta(\boldsymbol{x}\_i \mid \boldsymbol{z}\_i)$ highlighting the squared error in red:

$$\begin{align*} \text{loss}_{\text{VAE}}(\phi, \theta) &= -\sum_{i=1}^m E_{\boldsymbol{z}_i \sim q_\phi(\boldsymbol{z}_i \mid \boldsymbol{x}_i)} \left[\log p_\theta(\boldsymbol{x}_i \mid \boldsymbol{z}_i) \right] - KL(q_\phi(\boldsymbol{z}_i \mid \boldsymbol{x}_i) \ || \ p(\boldsymbol{z}_i) \\ &= -\sum_{i=1}^m E_{\boldsymbol{z}_i \sim q_\phi(\boldsymbol{z}_i \mid \boldsymbol{x}_i)} \left[\log \frac{1}{\sqrt{2 \pi \sigma_{\text{decoder}}^2}} - \frac{ \color{red}{||\boldsymbol{x}_i - f_\theta(\boldsymbol{z}_i) ||_2^2}}{2 \sigma_{\text{decoder}}^2} \right] - KL(q_\phi(\boldsymbol{z}_i \mid \boldsymbol{x}_i) \ || \ p(\boldsymbol{z}_i) \end{align*}$$

Recall, in the loss function for the standard autoencoder, we are also minimizing this squared error as seen below:

$$\text{loss}_{AE} := \frac{1}{m} \sum_{i=1}^m \color{red}{||\boldsymbol{x}_i - f_\theta(g_\phi(\boldsymbol{x}_i)) ||_2^2}$$

where $g_\phi$ is the encoding neural network and $f_\theta$ is the decoding neural network.

Thus, both the VAE and standard autoencoder will seek to minimize the squared error between the decoded data point $\boldsymbol{x}'_i$ and the original data point $\boldsymbol{x}_i$. In this regard, the two models are quite similar!

In constract to standard autoencoders, the VAE also has a KL-divergence term with opposite sign to the reconstruction loss term. Notice, how this term will push the model to generate latent variables, from $q\_\phi(\boldsymbol{z}\_i \mid \boldsymbol{x}\_i)$, that follow the prior distribution $p(\boldsymbol{z}\_i)$, which in our case is a standard normal. We can think of this KL-term as a **regularization term** for the reconstruction loss. That is, the model seeks to reconstruct each $\boldsymbol{x}\_i$; however, it also seeks to ensure that the latent $\boldsymbol{z}\_i$'s are distributed according to a standard normal distribution!

## Comparing the implementation of a VAE with that of an autoencoder

If our generative model assumes that $p\_\theta(\boldsymbol{x}\_i \mid \boldsymbol{z}\_i)$ is the normal distribution $N(f_\theta(\boldsymbol{z}), \sigma_{\text{decoder}}\boldsymbol{I})$, then the implementation of a standard autoencoder and a VAE are quite similar. To see this similarity, let's examine the computation graph of the loss function that we would use to train each model. For the standard autoencoder, the computation graph looks like:

&nbsp;

<center><img src="https://raw.githubusercontent.com/mbernste/mbernste.github.io/master/images/autoencoder.png" alt="drawing" width="350"/></center>

&nbsp;

PyTorch code for a simple autoencoder with one fully connected hidden layer in the encoder and decoder would look as follows:

```
class autoencoder(nn.Module):
    def __init__(
          self, 
          x_dim,
          hidden_dim,
          z_dim=10
        ):
        super(autoencoder, self).__init__()
        
        # Define autoencoding layers
        self.enc_layer1 = nn.Linear(x_dim, hidden_dim)
        self.enc_layer2 = nn.Linear(hidden_dim, z_dim) 

        # Define autoencoding layers
        self.dec_layer1 = nn.Linear(z_dim, hidden_dim)
        self.dec_layer2 = nn.Linear(hidden_dim, x_dim) 

    def encoder(self, x):
        # Define encoder network
        x = F.relu(self.enc_layer1(x))
        z = F.relu(self.enc_layer2(x))
        return z
        
    def decoder(self, z):
        # Define decoder network
        output = F.relu(self.dec_layer1(z))
        output = F.relu(self.dec_layer2(output))
        return output

    def forward(self, x):
        # Define the full network
        z = self.encoder(x)
        output = self.decoder(z)
        return output
```

For a VAE the computation graph looks like:

&nbsp;

<center><img src="https://raw.githubusercontent.com/mbernste/mbernste.github.io/master/images/VAE_computation_graph.png" alt="drawing" width="500"/></center>

&nbsp;

The PyTorch code for the autoencoder would then be slightly altered to include the sampling step:

```
class VAE(nn.Module):
    def __init__(
          self, 
          x_dim,
          hidden_dim,
          z_dim=10
        ):
        super(VAE, self).__init__()

        # Define autoencoding layers
        self.enc_layer1 = nn.Linear(x_dim, hidden_dim)
        self.enc_layer2_mu = nn.Linear(hidden_dim, z_dim) 
        self.enc_layer2_logvar = nn.Linear(hidden_dim, z_dim) 

        # Define autoencoding layers
        self.dec_layer1 = nn.Linear(z_dim, hidden_dim)
        self.dec_layer2 = nn.Linear(hidden_dim, x_dim) 

    def encoder(self, x):
        x = F.relu(self.enc_layer1(x))
        mu = F.relu(self.enc_layer2_mu(x))
        logvar = F.relu(self.enc_layer2_logvar(x))
        return mu, logVar

    def reparameterize(self, mu, logvar):
        std = torch.exp(logvar/2)
        eps = torch.randn_like(std)
        z = mu + std * eps
        return z

    def decoder(self, z):
        # Define decoder network
        output = F.relu(self.dec_layer1(z))
        output = F.relu(self.dec_layer2(output))
        return x

    def forward(self, x):
        mu, logvar = self.encoder(x)
        z = self.reparameterize(mu, logVar)
        output = self.decoder(z)
        return output, z, mu, logvar
```

Note that the output of the decoder, $\boldsymbol{x}'$, here can be interpreted to be the _mean_ of the distribution $p\_\theta(\boldsymbol{x}\_i \mid \boldsymbol{z}\_i)$. In practice,.

There are two main differences between the implementation of VAEs and autoencoders:
1. The loss function between the two is different (MSE versus the approximated ELBO)
2. We add steps to randomly sample $\boldsymbol{\epsilon}_i$ in order to generate $\boldsymbol{z}_i$ 

As will be shown later in this post, the code in PyTorch used to implement a simple VAE and autoencoder are quite similar.

Example: Training a VAE on MNIST
--------------------------------

I implemented a VAE in [PyTorch](https://pytorch.org) and trained it on the MNIST dataset. These data consist of 28x28 pixel images of hand written digits. My VAE used a latent representation of length 100 (that is, $\boldsymbol{z} \in \mathbb{R}^{100}$.

After enough training, the algorithm was able to reconstruct images that were not included in the training data. Here's an example:

&nbsp;

<center><img src="https://raw.githubusercontent.com/mbernste/mbernste.github.io/master/images/VAE_MNIST_reconstruction.png" alt="drawing" width="600"/></center>

&nbsp;

As you can see, it did pretty well! The reconstructed image looks very much like the original image. This is notable because the input image was not in the training set. Thus, the model learned how to generalize the task of encoding and decoding samples.

Let's see what happens when we attempt to reconstruct images from their latent representation. In order to do so, we first sample $\boldsymbol{z} \sim N(\boldsymbol{0}, \boldsymbol{I})$ and then display $f_\theta(\boldsymbol(z))$:

&nbsp;

<center><img src="https://raw.githubusercontent.com/mbernste/mbernste.github.io/master/images/example_MNIST_generative_process.png" alt="drawing" width="900"/></center>

&nbsp;

As you can see, some of these digits resemble numbers. Notably, nobody has actually written these digits. They were drawn by the model!

Lastly, let's explore the latent space learned by the model. First, let's take the images of a "9" and "0", and encode them into the latent space by sampling from $q_\phi(\boldsymbol{z} \mid \boldsymbol{x})$. Let's let $\boldsymbol{z}_1$ and $\boldsymbol{z}_2 be the latent vectors for "9" and "0" respectively: 

&nbsp;

<center><img src="https://raw.githubusercontent.com/mbernste/mbernste.github.io/master/images/VAE_encode_9_0.png" alt="drawing" width="650"/></center>

&nbsp;

Then, let's interpolate between $\boldsymbol{z}_1$ and $\boldsymbol{z}_2$ and for each interpolated vector $\boldsymbol{z}'$ we'll compute $f_\theta(\boldsymbol{z})$:  

&nbsp;

<center><img src="https://raw.githubusercontent.com/mbernste/mbernste.github.io/master/images/VAE_interpolate_9_to_0.png" alt="drawing" width="800"/></center>

&nbsp;


Interestingly, we see a smooth transition between the 9 to the 0. The latent representations that exist between 9 and 0 appear as sort of combined forms of these digits. 


Why VAEs over standard autoencoders?
------------------------------------


Appendix: Derivation of the KL-divergence term when the variational posterior and prior are Gaussian
----------------------------------------------------------------------------------------------------

If we choose the variational distribution $q_\phi(\boldsymbol{z} \mid \boldsymbol{x})$ to be defined by the following generative process:

$$\begin{align*}\boldsymbol{\mu} &:=  h^{(1)}_\phi(\boldsymbol{x}) \\ \log \boldsymbol{\sigma}^2 &:= h^{(2)}_\phi(\boldsymbol{x}) \\ \boldsymbol{z} &\sim N(\boldsymbol{\mu}, \boldsymbol{\sigma^2}\boldsymbol{I}) \end{align*}$$

where $h^{(1)}$ and $h^{(2)}$ are the two encoding neural networks, and we choose the prior distribution over $\boldsymbol{z}$ to be a standard normal distribution:

$$p(\boldsymbol{z}) := N(\boldsymbol{0}, \boldsymbol{I})$$

then the KL-divergence from $p(\boldsymbol{z})$ to $q_\phi(\boldsymbol{z} \mid \boldsymbol{x})$ is given by the following formula:

$$KL(q_\phi(\boldsymbol{z} \mid \boldsymbol{x}) \mid\mid p(\boldsymbol{z})) = -\frac{1}{2} \sum_{j=1}^J \left(1 + h^{(2)}_\phi(\boldsymbol{x})_j - \left(h^{(1)}_\phi(\boldsymbol{x})\right)_j^2 - \exp(h^{2}_\phi(\boldsymbol{x})_j) \right)$$

**Proof:**

First, let's re-write the KL-divergence as follows:

$$\begin{align*}KL(q_\phi(\boldsymbol{z} \mid \boldsymbol{x}) || p(\boldsymbol{z})) &= \int q_\phi(\boldsymbol{z} \mid \boldsymbol{x}) \log \frac{q_\phi(\boldsymbol{z} \mid \boldsymbol{x})}{ p(\boldsymbol{z}))} \\ &= \int q_\phi(\boldsymbol{z} \mid \boldsymbol{x}) \log q_\phi(\boldsymbol{z} \mid \boldsymbol{x}) \ d\boldsymbol{z} - q_\phi(\boldsymbol{z} \mid \boldsymbol{x}) \log p(\boldsymbol{z}) \ d\boldsymbol{z}\end{align*}$$

Now, let us compute the compute the first term, $\int q_\phi(\boldsymbol{z} \mid \boldsymbol{x}) \log q_\phi(\boldsymbol{z} \mid \boldsymbol{x}) \ d\boldsymbol{z}$:

$$\begin{align*}\int q_\phi(\boldsymbol{z} \mid \boldsymbol{x}) \log q_\phi(\boldsymbol{z} \mid \boldsymbol{x}) \ d\boldsymbol{z} &= \int N(\boldsymbol{z}; \boldsymbol{\mu}, \text{diag}(\boldsymbol{\sigma}^2)) \log N(\boldsymbol{z}; \boldsymbol{0}, \boldsymbol{I}))  \ d\boldsymbol{z} \\ &= \int N(\boldsymbol{z}; \boldsymbol{\mu}, \text{diag}(\boldsymbol{\sigma}^2)) \sum_{j=1}^J N(z_i; 0, 1) \ d\boldsymbol{z} \\ &= \int N(\boldsymbol{z}; \boldsymbol{\mu}, \text{diag}(\boldsymbol{\sigma}^2)) \sum_{j=1}^J \log \frac{1}{\sqrt{2\pi}} - \frac{1}{2} z_i^2 \ d\boldsymbol{z} \end{align*}$$

$\square$




