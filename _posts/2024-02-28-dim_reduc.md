---
title: 'Assessing the utility of data visualizations based on dimensionality reduction'
date: 2024-02-28
permalink: /posts/dim_reduc/
tags:
  - tutorial
  - data science
  - visualization
---

_THIS POST IS CURRENTLY UNDER CONSTRUCTION_

Introduction
------------

The advancement of technology has brought with it the ability to generate ever larger and more complex collections of data. This is especially true in biomedical research, where new technologies can produce thousands, or even millions, of biomolecular measurements at a time. Because we human beings use our vision as our chief sense for understanding the world, when we are confronted with data, we try to understand that data through visualization. Moreover, because we evolved in a three-dimensional world, we can only ever visualize up to three dimensions of an object at a time. This limitation poses a fundamental problem when it comes to high-dimensional data; high-dimensional data cannot, without loss of information, be visualized in their totality at once. But this does not mean we have not tried! The field of [dimension reduction algorithms](https://en.wikipedia.org/wiki/Dimensionality_reduction#:~:text=Dimensionality%20reduction%2C%20or%20dimension%20reduction,close%20to%20its%20intrinsic%20dimension.) studies and develops algorithms that map high dimensional data to two or three dimensions where we can visualize it with minimal loss of information. For example, the classical [principal components analysis (PCA)](https://en.wikipedia.org/wiki/Principal_component_analysis) uses a linear mapping to project data down to a space that preserves as much variance as possible. More recently, the [t-SNE](https://www.jmlr.org/papers/volume9/vandermaaten08a/vandermaaten08a.pdf) and [UMAP](https://arxiv.org/pdf/1802.03426.pdf) algorithms use [nonlinear mappings](https://en.wikipedia.org/wiki/Nonlinear_dimensionality_reduction) that attempts to preserve the "topology" of the data -- that is, that attempts to preserve neighborhoods of nearby data points while preventing overlapping dense regions of data in the output figure. An example of  [single-cell RNA-seq](https://en.wikipedia.org/wiki/Single-cell_sequencing) data from peripheral blood mononuclear cells (PBMCs) visualized by PCA, t-SNE, and UMAP are shown below (Data was downloaded via [Scanpy's](https://doi.org/10.1186/s13059-017-1382-0) [pbmc3k function](https://scanpy.readthedocs.io/en/latest/generated/scanpy.datasets.pbmc3k.html#scanpy.datasets.pbmc3k). Code to reproduce this figure can be found on [Google Colab](https://colab.research.google.com/drive/1g4bt9S0aE6qu8BZNvVblKNqIZf-UK62L?usp=sharing)):

&nbsp;

<center><img src="https://raw.githubusercontent.com/mbernste/mbernste.github.io/master/images/dim_reduc_PBMC_3k_example.png" alt="drawing" width="800"/></center>

&nbsp;

Unfortunately, because it is mathematically impossible to avoid losing information when mapping data from high to low dimensions, these algorithms inevitibly lose some aspect of the data, either by distortion or ommision, when plotting it in lower dimensions. This limitation makes the figures generated by these methods easy to misinterpret. Because of this, dimension reduction algorithms, especially t-SNE and UMAP, are facing [new scrutiny](https://journals.plos.org/ploscompbiol/article?id=10.1371/journal.pcbi.1011288) by those who argue that nonlinear dimension reduction algorithms distort the data so heavily that their output is at best useless and at worst harmful. On the other hand, proponents of these methods argue that although distortion is inevitable, these methods can and do reveal aspects of the data's structure that would be difficult to convey by other means.

In this blog post, I will attempt to provide my views on the matter which lie somewhere between those held by the critics and proponents. I will start with a review of what it means to "visualize" data. I will also review what it means to perform dimensionality reduction and how it inevitibly entails a loss of information. I will then argue that dimensionality reduction methods require a different kind of mentality to use them correctly than traditional data visualizations (i.e., those that do not compress high dimensional data into few dimensions). As a brief preview, I will argue that dimensionality reduction requires a "probabilistic" framework of interpretation rather than a "deterministic" one wherein conclusions one draws from a dimensionality reduction plot have some probability of not actually being true of the data. I will propose that this does not mean these plots are not useful! To evaluate their utility, I will argue that empirical [user studies](https://en.wikipedia.org/wiki/User_research) of these methods are required to evaluate them. That is, we must empirically assess whether or not the conclusions practitioners draw from these figures are more often true than not and, when not true, how consequential are they.

For much of this blog, I will use data generated by single-cell [RNA-sequencing](https://mbernste.github.io/posts/rna_seq_basics/) (scRNA-seq) as the primary example of high-dimensional data which I will use in a case study addressing the risks and merits of using dimension reduction for data visualization. For a comprehensive review on RNA-seq, please see [my previous blog post](https://mbernste.github.io/posts/rna_seq_basics/). 


Dimensionality reduction inevitably entails a loss of information
-----------------------------------------------------------------

In this section we will review the task of dimensionality reduction and describe why it inevitibly entails a loss of information. Before moving forward, let's formalize what we mean by the "dimensionality" of data. For the purposes of our discussion, we will refer to data as being $d$-dimensional if that data can be represented as a set of coordinate [vectors](https://mbernste.github.io/posts/vector_spaces/) in $\mathbb{R}^d$. That is, the dataset can be represented as $\boldsymbol{x}_1, \dots \boldsymbol{x}_n \in \mathbb{R}^d$. Collectively, we can represent the data as a [matrix](https://mbernste.github.io/posts/matrices/) $\boldsymbol{X}^{n \times d}$ where each row represents a datapoint. This description thus covers all tabular data. (For a more philsophical treatment on the notion of "dimensionality", see my [previous blog post](https://mbernste.github.io/posts/intrinsic_dimensionality/)).

The task of dimensionality reduction is to find a new set of vectors $\boldsymbol{x}'_1, \dots, \boldsymbol{x}'_2$ in a $d'$ dimensional space where $d' < d$ such that these lower dimensional points preserve some aspect of the original data's structure. Said more succintly, the task is to convert the high dimensional data $\boldsymbol{X} \in \mathbb{R}^{n \times d}$ to $\boldsymbol{X}' \in \mathbb{R}^{n \times d'}$ where $d' < d$. This is often cast as an optimization problem of the form:

$$\max_{\boldsymbol{X}' \in \mathbb{R}^{n \times d'}} \text{Similarity}(\boldsymbol{X}, \boldsymbol{X}')$$

where the function $\text{Similarity}(\boldsymbol{X}, \boldsymbol{X}')$ outputs a value that tells us "how well" the pairwise relationships between data points in $\boldsymbol{X}'$ reflect those in $\boldsymbol{X}$. 

For visualization purposes, if $d > 3$ we cannot easily visualize our data as a scatterplot to see the global structure between datapoints. Thus, it is common to set $d'$ to either 2 or 3 thus mapping each datapoint $\boldsymbol{x}_i$ to a new, 2 or 3 dimensional data point $\boldsymbol{x}'_i$ that can be visualized in a scatterplot. 

The problem, however, is that it is not possible (in general) to compress data down to a lower dimensional space and preserve all of the relative pairwise distances between data points. As an illustrative example, consider three points in 2-dimensional space that are equidistant from one another. If we were to compress these down into one dimension then _inevitably_ at least one pair of data points will have a larger distance from one another than the other two pairs. This is shown below:

<center><img src="https://raw.githubusercontent.com/mbernste/mbernste.github.io/master/images/dim_reduc_toy_example_equidistant.png" alt="drawing" width="600"/></center>

Notice how the distance between the blue and green data point is neither equal to the distance between the blue and red data points nor to the distance between the red and green data points as was the case in the original two dimensional space. Thus, the distances between this set of three data points have been distorted! 

This problem presents itself in the more familiar territory of creating maps. It is mathematically impossible to project the globe onto a 2D surface without distorting distances and/or shapes of continents. Different [map projections](https://en.wikipedia.org/wiki/Map_projection) have been devised to reflect certain aspects of the 3D configuration of the Earth's surface (e.g., shapes of continents), but that comes at the expense of some other aspect of that 3D configuration (e.g., distances between continents). A few examples of map projections are illustrated below (These images were created by [Daniel R. Strebe and were pulled from Wikipedia](https://en.wikipedia.org/wiki/List_of_map_projections)):

&nbsp;

<center><img src="https://raw.githubusercontent.com/mbernste/mbernste.github.io/master/images/globe_projections_by_Daniel_R_Strebe.png" alt="drawing" width="600"/></center>

&nbsp;

In fact, you can demonstrate this problem for yourself in your kitchen! Just peel an orange and attempt to lay the pieces of the orange peel on the table to reconstruct the original surface of the orange... it's impossible!

The loss of information inherent that follows dimensionality reduction is almost always an inevitibility. (There are scenarios where it is possible, but those tend to be uninteresting. An uninteresting example would be a case in which all of your data points lie in a 2-dimensional plane embedded in a higher-dimensional space). To visualize high dimensional data in two or three dimensions, one must either throw away dimensions and plot the remaining two/three or devise a more sophisticated approach that maps high dimensional data points to low dimensional data points to preserve _some aspect_ of the high dimensional data's structure (with respect to the Euclidean distances between data points) at the expense of other aspects. Exactly which aspect of the data's structure you wish to preserve depends on how you define your $\text{Similarity}(\boldsymbol{X}, \boldsymbol{X}')$ function described above! For example, PCA, t-SNE, and UMAP each have their own similarity functions that seek to preserve different aspects of the original data's structure.

Inferences are the primary product of a data visualization
----------------------------------------------------------

A probabistic framework for thinking about inferences generated by data visualizations
--------------------------------------------------------------------------------------

User studies may be the optimal way to assess the utility of dimensionality reduction plots
-------------------------------------------------------------------------------------------


