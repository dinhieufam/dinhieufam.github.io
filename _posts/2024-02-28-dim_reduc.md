---
title: 'Assessing the utility of data visualizations based on dimensionality reduction'
date: 2024-02-28
permalink: /posts/dim_reduc/
tags:
  - tutorial
  - data science
  - visualization
---

_THIS POST IS CURRENTLY UNDER CONSTRUCTION_

Introduction
------------

The advancement of technology has brought with it the ability to generate ever larger and more complex collections of data. This is especially true in biomedical research, where new technologies can produce thousands, or even millions, of biomolecular measurements at a time. Because we human beings use our vision as our chief sense for understanding the world, when we are confronted with data, we try to understand that data through visualization. Moreover, because we evolved in a three-dimensional world, we can only ever visualize up to three dimensions of an object at a time. This limitation poses a fundamental problem when it comes to high-dimensional data; high-dimensional data cannot, without loss of information, be visualized in their totality at once. But this does not mean we have not tried! The field of [dimension reduction algorithms](https://en.wikipedia.org/wiki/Dimensionality_reduction#:~:text=Dimensionality%20reduction%2C%20or%20dimension%20reduction,close%20to%20its%20intrinsic%20dimension.) studies and develops algorithms that map high dimensional data to two or three dimensions where we can visualize it with minimal loss of information. For example, the classical [principal components analysis (PCA)](https://en.wikipedia.org/wiki/Principal_component_analysis) uses a linear mapping to project data down to a space that preserves as much variance as possible. More recently, the [t-SNE](https://www.jmlr.org/papers/volume9/vandermaaten08a/vandermaaten08a.pdf) and [UMAP](https://arxiv.org/pdf/1802.03426.pdf) algorithms use [nonlinear mappings](https://en.wikipedia.org/wiki/Nonlinear_dimensionality_reduction) that attempts to preserve the "topology" of the data -- that is, that attempts to preserve neighborhoods of nearby data points while preventing overlapping dense regions of data in the output figure. An example of  [single-cell RNA-seq](https://en.wikipedia.org/wiki/Single-cell_sequencing) data from peripheral blood mononuclear cells (PBMCs) visualized by PCA, t-SNE, and UMAP are shown below (Data was downloaded via [Scanpy's](https://doi.org/10.1186/s13059-017-1382-0) [pbmc3k function](https://scanpy.readthedocs.io/en/latest/generated/scanpy.datasets.pbmc3k.html#scanpy.datasets.pbmc3k). Code to reproduce this figure can be found on [Google Colab](https://colab.research.google.com/drive/1g4bt9S0aE6qu8BZNvVblKNqIZf-UK62L?usp=sharing)):

&nbsp;

<center><img src="https://raw.githubusercontent.com/mbernste/mbernste.github.io/master/images/dim_reduc_PBMC_3k_example.png" alt="drawing" width="800"/></center>

&nbsp;

Unfortunately, because it is mathematically impossible to avoid losing information when mapping data from high to low dimensions, these algorithms inevitibly lose some aspect of the data, either by distortion or ommision, when plotting it in lower dimensions. This limitation makes the figures generated by these methods easy to misinterpret. Because of this, dimension reduction algorithms, especially t-SNE and UMAP, are facing [new scrutiny](https://journals.plos.org/ploscompbiol/article?id=10.1371/journal.pcbi.1011288) by those who argue that nonlinear dimension reduction algorithms distort the data so heavily that their output is at best useless and at worst harmful. On the other hand, proponents of these methods argue that although distortion is inevitable, these methods can and do reveal aspects of the data's structure that would be difficult to convey by other means.

In this blog post, I will attempt to provide my views on the matter which lie somewhere between those held by the critics and proponents. I will start with a review of what it means to "visualize" data. I will also review what it means to perform dimensionality reduction and how it inevitibly entails a loss of information. I will then argue that dimensionality reduction methods require a different kind of mentality to use them correctly than traditional data visualizations (i.e., those that do not compress high dimensional data into few dimensions). As a brief preview, I will argue that dimensionality reduction requires a "probabilistic" framework of interpretation rather than a "deterministic" one wherein conclusions one draws from a dimensionality reduction plot have some probability of not actually being true of the data. I will propose that this does not mean these plots are not useful! To evaluate their utility, I will argue that empirical [user studies](https://en.wikipedia.org/wiki/User_research) of these methods are required to evaluate them. That is, we must empirically assess whether or not the conclusions practitioners draw from these figures are more often true than not, and when not true, how consequential are they.

For much of this blog, I will use data generated by single-cell [RNA-sequencing](https://mbernste.github.io/posts/rna_seq_basics/) (scRNA-seq) as the primary example of high-dimensional data which I will use in a case study addressing the risks and merits of using dimension reduction for data visualization. For a comprehensive review on RNA-seq, please see [my previous blog post](https://mbernste.github.io/posts/rna_seq_basics/). 


Dimensionality reduction almost always entails a loss of information
--------------------------------------------------------------------

In this section we will review the task of dimensionality reduction and describe why it inevitibly entails a loss of information. Before moving forward, let's formalize what we mean by the "dimensionality" of data. For the purposes of our discussion, we will refer to data as being $d$-dimensional if that data can be represented as a set of coordinate [vectors](https://mbernste.github.io/posts/vector_spaces/) in $\mathbb{R}^d$. That is, the dataset can be represented as $\boldsymbol{x}_1, \dots \boldsymbol{x}_n \in \mathbb{R}^d$. Collectively, we can represent the data as a [matrix](https://mbernste.github.io/posts/matrices/) $\boldsymbol{X}^{n \times d}$ where each row represents a datapoint. This description thus covers all tabular data. (For a more philsophical treatment on the notion of "dimensionality", see my [previous blog post](https://mbernste.github.io/posts/intrinsic_dimensionality/)).

The task of dimensionality reduction is to find a new set of vectors $\boldsymbol{x}'_1, \dots, \boldsymbol{x}'_2$ in a $d'$ dimensional space where $d' < d$ such that these lower dimensional points preserve some aspect of the original data's structure. Said more succintly, the task is to convert the high dimensional data $\boldsymbol{X} \in \mathbb{R}^{n \times d}$ to $\boldsymbol{X}' \in \mathbb{R}^{n \times d'}$ where $d' < d$. This is often cast as an optimization problem of the form:

$$\max_{\boldsymbol{X}' \in \mathbb{R}^{n \times d'}} \text{Similarity}(\boldsymbol{X}, \boldsymbol{X}')$$

where the function $\text{Similarity}(\boldsymbol{X}, \boldsymbol{X}')$ outputs a value that tells us "how well" the pairwise relationships between data points in $\boldsymbol{X}'$ reflect those in $\boldsymbol{X}$. The exact form of $\text{Similarity}(\boldsymbol{X}, \boldsymbol{X}')$ depends on the dimensionality reduction method.

Note that if $d > 3$ then we cannot easily visualize our data as a scatterplot to see the global structure between datapoints. Thus, to visualize data it is common to set $d'$ to either 2 or 3 thereby mapping each datapoint $\boldsymbol{x}_i$ to a new, 2 or 3 dimensional data point $\boldsymbol{x}'_i$ that can be visualized in a scatterplot. 

However, there is a crucial problem to visualizing data in this manner: it is not possible (in general) to compress data down to a lower dimensional space and preserve all of the relative pairwise distances between data points. As an illustrative example, consider three points in 2-dimensional space that are equidistant from one another. If we were to compress these down into one dimension then _inevitably_ at least one pair of data points will have a larger distance from one another than the other two pairs. This is shown below:

<br>

<center><img src="https://raw.githubusercontent.com/mbernste/mbernste.github.io/master/images/dim_reduc_toy_example_equidistant.png" alt="drawing" width="600"/></center>

<br>

Notice how the distance between the blue and green data point is neither equal to the distance between the blue and red data points nor to the distance between the red and green data points as was the case in the original two dimensional space. Thus, the distances between this set of three data points have been distorted! 

This problem presents itself in the more familiar territory of creating maps. It is mathematically impossible to project the globe onto a 2D surface without distorting distances and/or shapes of continents. Different [map projections](https://en.wikipedia.org/wiki/Map_projection) have been devised to reflect certain aspects of the 3D configuration of the Earth's surface (e.g., shapes of continents), but that comes at the expense of some other aspect of that 3D configuration (e.g., distances between continents). A few examples of map projections are illustrated below (These images were created by [Daniel R. Strebe and were pulled from Wikipedia](https://en.wikipedia.org/wiki/List_of_map_projections)):

&nbsp;

<center><img src="https://raw.githubusercontent.com/mbernste/mbernste.github.io/master/images/globe_projections_by_Daniel_R_Strebe.png" alt="drawing" width="600"/></center>

&nbsp;

In fact, you can demonstrate this problem for yourself in your kitchen! Just peel an orange and attempt to lay the pieces of the orange peel on the table to reconstruct the original surface of the orange... it's impossible!

It is almost always the case that some information is lost following dimensionality reduction. To visualize high dimensional data in two or three dimensions, one must either throw away dimensions and plot the remaining two/three or devise a more sophisticated approach that maps high dimensional data points to low dimensional data points to preserve _some aspect_ of the high dimensional data's structure (with respect to the Euclidean distances between data points) at the expense of other aspects. Exactly which aspect of the data's structure you wish to preserve depends on how you define your $\text{Similarity}(\boldsymbol{X}, \boldsymbol{X}')$ function described above! (Note, there are scenarios where it is possible to preserve pairwise distances between data points following dimensionality reduction, but those scenarios tend to be uninteresting. An uninteresting example would be a case in which all of your data points lie in a 2-dimensional plane embedded in a higher-dimensional space). 

A probabistic framework for thinking about inferences generated by data visualizations
--------------------------------------------------------------------------------------

Before we further discuss the dangers and merits of dimensionality reduction, let us discuss data visualization more generally. Now, here I am going to make a statement that may appear obvious, but is important for laying the foundation for my views on dimensionality reduction specifically: the primary output of a data visualization are a set of facts about the data. Take the following bar chart as an example to describe what I mean by this statement:


One fact about the data being conveyed by this plot is that Label A is associated with the value 9. Another fact about the data is that Label A is associated with a larger value than Label B. Note, I am describing facts _about the data_, not facts about the _world_. That is, the barchart is only making a claim about the literal values stored in the data table from which this figure was generated. (The task of drawing conclusions about the world based on data and/or data visualizations is addressed by the field of statistics). Data visualizations make facts about the data easier to understand than the raw data by itself (i.e., large tables of numbers) because we human beings are visual animals.

For traditionaly data visualizations, facts about the data being described by the visualization are 100% certain to be true. For example, when one looks at the barchart above they _know_ that Label A is associated with a larger value than Label B's value (unless of course, there was an error in the generation of the visualization, but we will assume no errors were made here). That is because in traditional data visualizations, there is a linear mapping between some aspect of the data and some visual or spatial element to the visualization.   









User studies may be the optimal way to assess the utility of dimensionality reduction plots
-------------------------------------------------------------------------------------------


