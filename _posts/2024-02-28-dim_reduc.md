---
title: 'Assessing the utility of data visualizations based on dimensionality reduction'
date: 2024-02-28
permalink: /posts/dim_reduc/
tags:
  - tutorial
  - data science
  - visualization
---

_THIS POST IS CURRENTLY UNDER CONSTRUCTION_

Introduction
------------

The advancement of technology has brought with it the ability to generate ever larger and more complex collections of data. This is especially true in biomedical research, where new technologies can produce thousands, or even millions, of biomolecular measurements at a time. Because we human beings use our vision as our chief sense for understanding the world, when we are confronted with data, we try to understand that data through visualization. Moreover, because we evolved in a three-dimensional world, we can only ever visualize up to three dimensions of an object at a time. This limitation poses a fundamental problem when it comes to high-dimensional data; high-dimensional data cannot, without loss of information, be visualized in their totality at once. But this does not mean we have not tried! The field of [dimension reduction algorithms](https://en.wikipedia.org/wiki/Dimensionality_reduction#:~:text=Dimensionality%20reduction%2C%20or%20dimension%20reduction,close%20to%20its%20intrinsic%20dimension.) studies and develops algorithms that map high dimensional data to two or three dimensions where we can visualize it with minimal loss of information. For example, the classical [principal components analysis (PCA)](https://en.wikipedia.org/wiki/Principal_component_analysis) uses a linear mapping to project data down to a space that preserves as much variance as possible. More recently, the [t-SNE](https://www.jmlr.org/papers/volume9/vandermaaten08a/vandermaaten08a.pdf) and [UMAP](https://arxiv.org/pdf/1802.03426.pdf) algorithms use [nonlinear mappings](https://en.wikipedia.org/wiki/Nonlinear_dimensionality_reduction) that attempts to preserve the "topology" of the data -- that is, that attempts to preserve neighborhoods of nearby data points while preventing overlapping dense regions of data in the output figure. An example of  [single-cell RNA-seq](https://en.wikipedia.org/wiki/Single-cell_sequencing) data from peripheral blood mononuclear cells (PBMCs) visualized by PCA, t-SNE, and UMAP are shown below (Data was downloaded via [Scanpy's](https://doi.org/10.1186/s13059-017-1382-0) [pbmc3k function](https://scanpy.readthedocs.io/en/latest/generated/scanpy.datasets.pbmc3k.html#scanpy.datasets.pbmc3k). Code to reproduce this figure can be found on [Google Colab](https://colab.research.google.com/drive/1g4bt9S0aE6qu8BZNvVblKNqIZf-UK62L?usp=sharing)):

&nbsp;

<center><img src="https://raw.githubusercontent.com/mbernste/mbernste.github.io/master/images/dim_reduc_PBMC_3k_example.png" alt="drawing" width="800"/></center>

&nbsp;

Unfortunately, because it is mathematically impossible to avoid losing information when mapping data from high to low dimensions, these algorithms inevitibly lose some aspect of the data, either by distortion or ommision, when plotting it in lower dimensions. This limitation makes the figures generated by these methods easy to misinterpret. Because of this, dimension reduction algorithms, especially t-SNE and UMAP, are facing [new scrutiny](https://journals.plos.org/ploscompbiol/article?id=10.1371/journal.pcbi.1011288) by those who argue that nonlinear dimension reduction algorithms distort the data so heavily that their output is at best useless and at worst harmful. On the other hand, proponents of these methods argue that although distortion is inevitable, these methods can and do reveal aspects of the data's structure that would be difficult to convey by other means.

In this blog post, I will attempt to provide my views on the matter which lie somewhere between those held by the critics and proponents. I will start with a review of what it means to "visualize" data. I will also review what it means to perform dimensionality reduction and how it inevitibly entails a loss of information. I will then argue that dimensionality reduction methods require a different kind of mentality to use them correctly than traditional data visualizations (i.e., those that do not compress high dimensional data into few dimensions). As a brief preview, I will argue that dimensionality reduction requires a "probabilistic" framework of interpretation rather than a "deterministic" one wherein conclusions one draws from a dimensionality reduction plot have some probability of not actually being true of the data. I will propose that this does not mean these plots are not useful! To evaluate their utility, I will argue that empirical [user studies](https://en.wikipedia.org/wiki/User_research) of these methods are required to evaluate them. That is, we must empirically assess whether or not the conclusions practitioners draw from these figures are more often true than not, and when not true, how consequential are they.

For much of this blog, I will use data generated by single-cell [RNA-sequencing](https://mbernste.github.io/posts/rna_seq_basics/) (scRNA-seq) as the primary example of high-dimensional data which I will use in a case study addressing the risks and merits of using dimension reduction for data visualization. For a comprehensive review on RNA-seq, please see [my previous blog post](https://mbernste.github.io/posts/rna_seq_basics/). 


Dimensionality reduction almost always entails a loss of information
--------------------------------------------------------------------

In this section we will review the task of dimensionality reduction and describe why it inevitibly entails a loss of information. Before moving forward, let's formalize what we mean by the "dimensionality" of data. For the purposes of our discussion, we will refer to data as being $d$-dimensional if that data can be represented as a set of coordinate [vectors](https://mbernste.github.io/posts/vector_spaces/) in $\mathbb{R}^d$. That is, the dataset can be represented as $\boldsymbol{x}_1, \dots \boldsymbol{x}_n \in \mathbb{R}^d$. Collectively, we can represent the data as a [matrix](https://mbernste.github.io/posts/matrices/) $\boldsymbol{X}^{n \times d}$ where each row represents a datapoint. This description thus covers all tabular data. (For a more philsophical treatment on the notion of "dimensionality", see my [previous blog post](https://mbernste.github.io/posts/intrinsic_dimensionality/)).

The task of dimensionality reduction is to find a new set of vectors $\boldsymbol{x}'_1, \dots, \boldsymbol{x}'_2$ in a $d'$ dimensional space where $d' < d$ such that these lower dimensional points preserve some aspect of the original data's structure. Said more succintly, the task is to convert the high dimensional data $\boldsymbol{X} \in \mathbb{R}^{n \times d}$ to $\boldsymbol{X}' \in \mathbb{R}^{n \times d'}$ where $d' < d$. This is often cast as an optimization problem of the form:

$$\max_{\boldsymbol{X}' \in \mathbb{R}^{n \times d'}} \text{Similarity}(\boldsymbol{X}, \boldsymbol{X}')$$

where the function $\text{Similarity}(\boldsymbol{X}, \boldsymbol{X}')$ outputs a value that tells us "how well" the pairwise relationships between data points in $\boldsymbol{X}'$ reflect those in $\boldsymbol{X}$. The exact form of $\text{Similarity}(\boldsymbol{X}, \boldsymbol{X}')$ depends on the dimensionality reduction method.

Note that if $d > 3$ then we cannot easily visualize our data as a scatterplot to see the global structure between datapoints. Thus, to visualize data it is common to set $d'$ to either 2 or 3 thereby mapping each datapoint $\boldsymbol{x}_i$ to a new, 2 or 3 dimensional data point $\boldsymbol{x}'_i$ that can be visualized in a scatterplot. 

However, there is a crucial problem to visualizing data in this manner: it is not possible (in general) to compress data down to a lower dimensional space and preserve all of the relative pairwise distances between data points. As an illustrative example, consider three points in 2-dimensional space that are equidistant from one another. If we were to compress these down into one dimension then _inevitably_ at least one pair of data points will have a larger distance from one another than the other two pairs. This is shown below:

<br>

<center><img src="https://raw.githubusercontent.com/mbernste/mbernste.github.io/master/images/dim_reduc_toy_example_equidistant.png" alt="drawing" width="600"/></center>

<br>

Notice how the distance between the blue and green data point is neither equal to the distance between the blue and red data points nor to the distance between the red and green data points as was the case in the original two dimensional space. Thus, the distances between this set of three data points have been distorted! 

This problem presents itself in the more familiar territory of creating maps. It is mathematically impossible to project the globe onto a 2D surface without distorting distances and/or shapes of continents. Different [map projections](https://en.wikipedia.org/wiki/Map_projection) have been devised to reflect certain aspects of the 3D configuration of the Earth's surface (e.g., shapes of continents), but that comes at the expense of some other aspect of that 3D configuration (e.g., distances between continents). A few examples of map projections are illustrated below (These images were created by [Daniel R. Strebe and were pulled from Wikipedia](https://en.wikipedia.org/wiki/List_of_map_projections)):

&nbsp;

<center><img src="https://raw.githubusercontent.com/mbernste/mbernste.github.io/master/images/globe_projections_by_Daniel_R_Strebe.png" alt="drawing" width="600"/></center>

&nbsp;

In fact, you can demonstrate this problem for yourself in your kitchen! Just peel an orange and attempt to lay the pieces of the orange peel on the table to reconstruct the original surface of the orange... it's impossible!

It is almost always the case that some information is lost following dimensionality reduction. To visualize high dimensional data in two or three dimensions, one must either throw away dimensions and plot the remaining two/three or devise a more sophisticated approach that maps high dimensional data points to low dimensional data points to preserve _some aspect_ of the high dimensional data's structure (with respect to the Euclidean distances between data points) at the expense of other aspects. Exactly which aspect of the data's structure you wish to preserve depends on how you define your $\text{Similarity}(\boldsymbol{X}, \boldsymbol{X}')$ function described above! (Note, there are scenarios where it is possible to preserve pairwise distances between data points following dimensionality reduction, but those scenarios tend to be uninteresting. An uninteresting example would be a case in which all of your data points lie in a 2-dimensional plane embedded in a higher-dimensional space). 

A probabistic mindset for thinking about inferences drawn from data visualizations
----------------------------------------------------------------------------------

Now, I am going to build a framework for thinking about data visualization that will draw what I view as an important distinction between "traditional" data visualizations, such as heatmaps or barcharts, and data visualizations based on dimensionality reduction such as UMAP scatterplots. As a very brief preview, I will argue that traditional data visualizations enable one to make claims about the underlying data with 100% certainty whereas dimensionality reduction visualizations do not provide the same certainty.

Before we get going, I am going to make a statement that may appear obvious, but is important for laying the foundation for my views on dimensionality reduction: **The primary outputs of a data visualization are a set of statements about the data**. Take the following table and associated bar chart as an example to describe what I mean by this:

<br>
<center><img src="https://raw.githubusercontent.com/mbernste/mbernste.github.io/master/images/dim_reduc_traditional_data_viz.png" alt="drawing" width="600"/></center>

<br>

One statement about the data being conveyed by this plot is that Label A is associated with the value 9. Another statement about the data is that Label A is associated with a larger value than Label B. Below are a set of example statements about the data being conveyed by this plot:

<br>

<center><img src="https://raw.githubusercontent.com/mbernste/mbernste.github.io/master/images/dim_reduc_statements_on_barchart.png" alt="drawing" width="450"/></center>

<br>

Note, I am describing statements _about the data_, not statements about the _world_. That is, the barchart enables one to make claims about the literal values stored in the data table from which this figure was generated. (The task of drawing conclusions about the world based on data and/or data visualizations is the task of science and statistics). Data visualizations make facts about the data easier to understand than the raw data by itself (i.e., large tables of numbers) because we human beings are visual animals.

For traditionaly data visualizations, statements about the data being described by the visualization are 100% certain to be true. For example, when one looks at the barchart above they _know_ that Label A is associated with a larger value than Label B's value (unless of course, there was an error in the generation of the visualization, but we will assume no errors were made here). That is because in traditional data visualizations, there is either a one-to-one or linear mapping between some aspect of the data and some visual or spatial element to the visualization. In the bar chart above, the mappings are as follows:

* Magnitude of Value $\rightarrow$ Height of bar (linear mapping)
* Distinct Label $\rightarrow$ Distinct bar (one-to-one mapping)

Because these mappings are [invertible](https://en.wikipedia.org/wiki/Inverse_function) and [deterministic](https://en.wikipedia.org/wiki/Deterministic_system), we can draw conclusions about the raw data with 100% certainty based on the visual elements in the figure.

Now, let's turn our attention to data visualizations produced by dimensionality reduction methods. Let's use a UMAP plot of the PBMCs shown above, but now let's not color the cells by their respective cell type and let's pretend we don't know much of anything about these cells. 

<center><img src="https://raw.githubusercontent.com/mbernste/mbernste.github.io/master/images/dim_reduc_PBMC_uncolored.png" alt="drawing" width="600"/></center>

What are some statements we might make about the data from this figure? Below are a few examples:


The problem is that many of the above statements are probably not true! For UMAP in particular, statements regarding distances are [especially unlikely to be true](https://journals.plos.org/ploscompbiol/article?id=10.1371/journal.pcbi.1011288). This is because, as we discussed before, dimensionality reduction methods distort the data. Statements we make about the data from this figure do not have the same gaurantee of truth as statements we made from the bar chart! 

This brings us to a probablistic framework for thinking about data visualizations. Any statement about a dataset, $S$, drawn from a traditional data visualization, like the bar chart, has the following property:

$$P(S = \text{True}) = 1$$

On the other hand, this is not the case for dimensionality reduction plots. Rather,

$$P(S = \text{True}) < 1$$

Of course, the probability $P(S = \text{True})$ cannot really be defined in a [frequentist sense](https://en.wikipedia.org/wiki/Frequentist_probability), since $S$ either is or is not true. Rather, this is more of a [Bayesian probability](https://en.wikipedia.org/wiki/Bayesian_probability) (i.e., a degree of certainty) that can be informed by looking across many plots with a similar feature as the feature being described by $S$ and asking: for what fraction of the datasets described by those plots is the statement $S$ true? 

I concede that this "probabilistic framework" is a bit hand-wavey and not very rigorous, but to me illustrates an important distinction between plots based on dimensionality reduction and "traditional" data visualizations. Specifically, for dimensionality reduction plots, the statements one might draw from them about the data may be wrong. Said differently, dimensionality reduction plots help users draw _uncertain inferences_ about the structure of the data whereas there is no uncertainty in a traditional data visualization.

Expert users of dimensionality reduction plots know this, of course, but use them anyway. Why? Well, because they claim that certain "classes" of statements are more often true than not and because of this fact, these methods are useful. For example, when it comes to UMAP, it is _often_ true (but not always) that when you see distinct clusters in the scatterplot, those clusters are real characteristics of the data. Thus, one might say that for statements on clusters, $S_{\text{cluster}}$, $P(S_{\text{cluster}} = \text{True})$ is high enough to be useful. On the other hand, for statments on distances (especially long distances) between data points, $S_{\text{distance}}$, $P(S_{\text{distance}} = \text{True})$ is far too low to be useful. The problem then is to determine which statements are more likely to be true than others for a given dimensionality reduction method. 


Addressing the criticisms of dimensionality reduction methods
-------------------------------------------------------------

As I mentioned in the introduction, dimensionality reduction methods are receiving new scrutiny. Given my "probabilistic mindset" for interpreting data visualizations, I will attempt to summarize my understanding of certain criticisms of dimensionality reduction methods, especially non-linear methods like t-SNE and UMAP:

**Concern 1: Distortion caused by popular methods like t-SNE and UMAP are too severe to be useful:** That is, distortion of the data is so severe that practically any interesting statement $S$ that one might make from such a visualization is associated with too low of a probability of actually being true about the data to be useful for anything.

To this concern, I am undecided. As I will discuss in the next section, I believe that the best way to assess dimensionality reduction figures is to use human studies. More specifically, _statements_ about the data are the _primary output of a data visualization_. Because of this, we should be evaluating those statements that are made by practictioners of these methods in the field. That is, even though these methods distort the data, to me it remains an open question on whether or not the statements that practicioners draw from these plots _in practice_ have a high or low probability of being true. Do these plots lead to new insights in practice despite the fact that not all statements drawn from them are true? What alternative visualizations would provide the same insights?

**Concern 2: We don't know what inferences can be drawn reliably from these plots:** That is, we do not have a deep enough understanding into the classes of statements that have high or low probability of being true.



**Concern 3: Any visualization in which there is uncertainty around what it says about the data should be avoided:** The argument here is that it is too easy to misinterpret and misuse _any_ data visualization technique in which one can make a reasonable statement about the data, $S$, but that $P(S = \text{True}) < 1$. 

The concern here is that if a plot does not provide certainty into the data that it describes, then it is too easy to fall victim to confirmation bias when interpreting that figure. I admit I fell prey to this myself. In [a paper I worked](https://doi.org/10.1016/j.isci.2020.101913) presenting a cell type classification algorithm, called [CellO](https://doi.org/10.1016/j.xpro.2021.100705), I made the following statement based on a UMAP plot (referencing Figure 7): "CellO annotated many of these cells as pancreatic A cells (a.k.a. pancreatic alpha cells), which is plausible owing to both their close position to annotated A cells according to UMAP, which is known to preserve some level of global structure in high dimensional data (Becht et al., 2018)..." Granted, this statement is not very strong, I nonetheless ask myself whether what I saw in that UMAP plot is what I wanted to see? Indeed, because these figures may make us more prone to confirmation bias, I am sympathetic to the argument that we should avoid them altogether. At the very least, one should use extreme caution when using them and make sure to confirm any hypotheses generated from these figures using orthogonal techniques.


User studies may be the optimal way to assess the utility of dimensionality reduction plots
-------------------------------------------------------------------------------------------
